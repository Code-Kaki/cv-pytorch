{"cells":[{"cell_type":"markdown","source":["<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n","\n","> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n"," <br>\n","This program and the accompanying materials are made available under the\n","terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n","WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n","License for the specific language governing permissions and limitations\n","under the License. <br>\n","<br>**SPDX-License-Identifier: Apache-2.0** "],"metadata":{"id":"QZ8_rATdS42e"},"id":"QZ8_rATdS42e"},{"cell_type":"markdown","metadata":{"id":"l5ciXi14NPWh"},"source":["# Demo - Predict Car Price \n","Authored by : [Nazurah Kamil](mailto:nazurah.kamil@skymind.my)"],"id":"l5ciXi14NPWh"},{"cell_type":"markdown","metadata":{"id":"lLBBOxdCNPWi"},"source":["In [Feature Engineering](../../machine_learning/supervised_learning/Feature%20Engineering.ipynb), we have built a `LinearRegression` using `scikit-learn` library to predict car price. This notebook will instead use neural network to attempt the same problem.\n","\n","###### Learning Outcome\n","By the end of this notebook, you will be able to:\n","- Apply deep learning model to perform regression modelling\n","- Utilize `Dataset` and `Dataloader`\n","- Determine how well our deep learning model generalize on test set"],"id":"lLBBOxdCNPWi"},{"cell_type":"markdown","metadata":{"id":"Tyl87t1aNPWi"},"source":["Let us first import the required libraries for this notebook."],"id":"Tyl87t1aNPWi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"nq2zRjBoNPWj"},"outputs":[],"source":["import pandas as pd\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.tensorboard import SummaryWriter\n","%reload_ext tensorboard\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"id":"nq2zRjBoNPWj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgeU06JyNPWj"},"outputs":[],"source":["# Import dataset\n","import pandas as pd\n","car_data = pd.read_csv(\"https://raw.githubusercontent.com/Saranya-Skymind/Datasets/main/car%20price.csv\")"],"id":"mgeU06JyNPWj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"_g-rYLlwNPWj"},"outputs":[],"source":["car_data.shape"],"id":"_g-rYLlwNPWj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIhtbkZkNPWk"},"outputs":[],"source":["car_data.head()"],"id":"DIhtbkZkNPWk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDw5AZ3yNPWk"},"outputs":[],"source":["# Remove unused column\n","car_data.drop(['CarName','car_ID'], axis=1, inplace=True)"],"id":"mDw5AZ3yNPWk"},{"cell_type":"code","source":["car_data.shape"],"metadata":{"id":"WqrhNmOmYth-"},"id":"WqrhNmOmYth-","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtsqhkeHNPWl"},"source":["## Feature Engineering"],"id":"xtsqhkeHNPWl"},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6TOCW4YNPWl"},"outputs":[],"source":["# Change from string to int\n","word2num = {\"doornumber\": {\"two\": 2, \"four\": 4},\n","            \"cylindernumber\": {\"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n","                               \"six\": 6, \"eight\": 8, \"twelve\": 12}}\n","\n","# Replace the values in dataset\n","car_data = car_data.replace(word2num)\n","car_data.dtypes"],"id":"m6TOCW4YNPWl"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLJJ1TUKNPWl"},"outputs":[],"source":["car_data['fueltype'].unique()"],"id":"uLJJ1TUKNPWl"},{"cell_type":"markdown","metadata":{"id":"vhdYCFDpNPWl"},"source":["## **Dummy Encoding**\n","Let's use dummy encoding to encode our categorical features in string format into binary format."],"id":"vhdYCFDpNPWl"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxASLby6NPWm"},"outputs":[],"source":["# Dummy encoding to categorical features\n","car_encode = pd.get_dummies(car_data, columns=['fueltype', 'aspiration', 'carbody', 'drivewheel', 'enginelocation',\n","                                      'enginetype', 'fuelsystem'], drop_first=True)\n","car_encode.head()"],"id":"jxASLby6NPWm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpkQZ3PLNPWm"},"outputs":[],"source":["car_encode.columns.tolist().index('fueltype_gas')"],"id":"HpkQZ3PLNPWm"},{"cell_type":"markdown","metadata":{"id":"sCplpk7pNPWm"},"source":["## Features and Label\n","Now, we will separate out the features and store it in variable `X` and store label in variable `y`."],"id":"sCplpk7pNPWm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-R1BXr4PNPWm"},"outputs":[],"source":["X = car_encode.drop('price', axis=1)\n","X.head()"],"id":"-R1BXr4PNPWm"},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"Vh-YF1AKYGSb"},"id":"Vh-YF1AKYGSb","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cR3Iy_eFNPWn"},"outputs":[],"source":["y = car_encode.price\n","y"],"id":"cR3Iy_eFNPWn"},{"cell_type":"markdown","metadata":{"id":"FH1RF9FXNPWn"},"source":["# Split to Train And Test"],"id":"FH1RF9FXNPWn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"925JCqdgNPWn"},"outputs":[],"source":["# split train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=5)"],"id":"925JCqdgNPWn"},{"cell_type":"markdown","metadata":{"id":"VMkEZYq3NPWn"},"source":["# Data Preprocessing"],"id":"VMkEZYq3NPWn"},{"cell_type":"markdown","metadata":{"id":"Oql2z7P7NPWn"},"source":["Next, we are going to perform feature scaling on `X_train` and `X_test` using `StandardScaler` from `scikit-learn`.<br>\n","*Note: only fit the train features but transform both train and test features*"],"id":"Oql2z7P7NPWn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK_bVpXoNPWo"},"outputs":[],"source":["# feature scaling\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","Y_train=scaler.fit_transform(Y_train)"],"id":"vK_bVpXoNPWo"},{"cell_type":"markdown","metadata":{"id":"XjSK15nWNPWo"},"source":["# Dataset And DataLoader"],"id":"XjSK15nWNPWo"},{"cell_type":"markdown","metadata":{"id":"RyEsfE7oNPWo"},"source":["Here, we are using a custom dataset from a `csv` file. Thus, we have to build our own `Dataset` class by subclassing from `torch.utils.data.Dataset`."],"id":"RyEsfE7oNPWo"},{"cell_type":"markdown","metadata":{"id":"Kn_TMPiRNPWo"},"source":["Whilst subclassing `Dataset`, PyTorch [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) states that we have to override the `__getitem__()` method and optionally the `__len__()` method.<br>\n","We will mainly have three methods in this `Dataset` class:\n","- `__init__(self, data, label)`: helps us pass in the feature and labels into the dataset\n","- `__len__(self)`:allows the dataset to know how many instances of data there is \n","- `__getitem__(self, idx)`:allows the dataset to get items from the data and labels by indexing"],"id":"Kn_TMPiRNPWo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqQlnYcjNPWo"},"outputs":[],"source":["class Custom_Dataset(Dataset):\n","    def __init__(self, features, labels):\n","        # convert dataset to tensor\n","        self.features = torch.tensor(features, dtype = torch.float32)\n","        self.labels = torch.tensor(labels.values, dtype  = torch.float32)\n","\n","    def __len__(self):\n","        # return len of features\n","        return self.features.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        return self.features[idx], self.labels[idx]"],"id":"tqQlnYcjNPWo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDvriYTNNPWo"},"outputs":[],"source":["train_dataset = Custom_Dataset(X_train, y_train)\n","test_dataset = Custom_Dataset(X_test, y_test)"],"id":"yDvriYTNNPWo"},{"cell_type":"markdown","metadata":{"id":"p9An99wpNPWp"},"source":["`DataLoader` helps us to transform our dataset into an iterable dataset and allows for batch loading with a configurable size (batch size). It can also be shuffled before loading, which helps in randomizing the input. This allows for faster optimization to minimize loss."],"id":"p9An99wpNPWp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsghvkLvNPWp"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size = 32)\n","test_loader = DataLoader(test_dataset, batch_size = 64 )"],"id":"vsghvkLvNPWp"},{"cell_type":"markdown","metadata":{"id":"y7wmty9INPWp"},"source":["`torch.nn.Sequential` is a function that accepts a list of `nn.Modules` and returns a model with all the sequential layers. We will be configuring these few layers:\n","1. nn.Linear(38,50)\n","2. nn.ReLU()\n","3. nn.Linear(50,25)\n","4. nn.ReLU()\n","5. nn.Linear(25,10)\n","6. nn.ReLU()\n","7. nn.Linear(10,1)"],"id":"y7wmty9INPWp"},{"cell_type":"markdown","metadata":{"id":"0Q0qylF0NPWp"},"source":["# Define Model"],"id":"0Q0qylF0NPWp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCr6uTkjNPWp"},"outputs":[],"source":["n_features = 38\n","num_output = 1\n","torch.manual_seed(123)\n","model_sequential = nn.Sequential(nn.Linear(n_features, 50),\n","                                 nn.ReLU(),\n","                                 nn.Linear(50, 25),\n","                                 nn.ReLU(),\n","                                 nn.Linear(25, 10),\n","                                 nn.ReLU(),\n","                                 nn.Linear(10, num_output)\n","                                 )"],"id":"qCr6uTkjNPWp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYSY6jD1NPWp"},"outputs":[],"source":["# Loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model_sequential.parameters(), lr = 0.01)\n","\n","# Tensorboard\n","writer = SummaryWriter('run/LinearRegression')"],"id":"iYSY6jD1NPWp"},{"cell_type":"markdown","metadata":{"id":"BzWsxON1NPWp"},"source":["# Start Model Training"],"id":"BzWsxON1NPWp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrFLz-qANPWq"},"outputs":[],"source":["# Perform model training on training data only\n","losses_list = []\n","def train_model(epochs, model, criterion, optimizer, loader, writer):\n","    running_loss = 0.0\n","    \n","    # Repeat for given epoch numbers\n","    for epoch in range(1, epochs+1):\n","        \n","        # Train with mini-batch of data\n","        for i, data in enumerate(loader):\n","            \n","            # clearing the gradient from previous minibatch gradient computation\n","            optimizer.zero_grad()\n","            \n","            # divide into features and labels\n","            features, labels = data[0], data[1]\n","            \n","            # predict using features\n","            prediction = model(features)\n","            \n","            # RMSE loss\n","            loss = torch.sqrt(criterion(prediction, torch.unsqueeze(labels, dim=1)))\n","            \n","            # Compute gradient\n","            loss.backward()\n","            \n","            # Updating weight and bias\n","            optimizer.step()\n","            \n","            # this running_loss will keep track of the losses of every epoch from each respective iteration\n","            running_loss += loss.item()\n","        \n","        loss_per_epoch = running_loss / len(loader) # mini-batch size \n","        \n","        # Print the progress (for this print every 10 epochs)\n","        if (epoch % 10 == 0 or epoch == 1):\n","            print(f\"Epoch {epoch} Train Loss: {loss_per_epoch}\")\n","        running_loss = 0.0\n","        writer.add_scalar('Loss', loss_per_epoch, epoch)\n","        losses_list.append(loss_per_epoch)\n","    writer.close()"],"id":"nrFLz-qANPWq"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"j1hJ227fNPWq"},"outputs":[],"source":["num_epochs = 100\n","train_model(num_epochs, model_sequential, criterion, optimizer, train_loader, writer)"],"id":"j1hJ227fNPWq"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wP8GW3YyNPWq"},"outputs":[],"source":["# Visualizing the loss curve\n","plt.figure(figsize=(10,6))\n","plt.plot(range(num_epochs), losses_list);\n","plt.grid()\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.show()"],"id":"wP8GW3YyNPWq"},{"cell_type":"markdown","metadata":{"id":"a_GRSg7HNPWr"},"source":["From the graph, we can observe the loss value of each epoch. After 20 epochs, there is not much difference in loss for the subsequent epochs. The effect in further training beyond 20 epochs is not too significant for loss score improvement. Next, we will see how good our model generalize to test set."],"id":"a_GRSg7HNPWr"},{"cell_type":"markdown","metadata":{"id":"5MReEweiNPWr"},"source":["There is no point of computing gradient during model testing. So, let's call `torch.no_grad` before evaluating our model using test set.\n","\n","**`torch.no_grad`**<br>\n","`torch.no_grad` sets the tensor's `reguires_grad` property to `False` and turns off the `Autograd` engine, which computes gradients with respect to parameters. We do not need any gradients in the test step because the parameter updates were done in the training step, hence this context manager is utilized the test phase only."],"id":"5MReEweiNPWr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1Q0ueFmNPWr"},"outputs":[],"source":["# Set our model to evaluation mode (model.eval())\n","model_sequential.eval()\n","\n","# Test set\n","with torch.no_grad():\n","    running_loss = 0.0\n","    \n","    for idx, (X_test, y_test) in enumerate(test_loader):\n","        y_predtest = model_sequential(X_test)\n","        \n","        loss = torch.sqrt(criterion(y_predtest, torch.unsqueeze(y_test, dim=1)))\n","        \n","        running_loss += loss.item() * y_predtest.size(0)\n","        \n","    print(f'Test Loss: {running_loss / len(X_test)}')  "],"id":"A1Q0ueFmNPWr"},{"cell_type":"markdown","metadata":{"id":"IiUO4FuRNPWt"},"source":["It seem that our model can generalize to the test dataset pretty well."],"id":"IiUO4FuRNPWt"},{"cell_type":"markdown","metadata":{"id":"fcysb3M7NPWt"},"source":["# Tensorboard"],"id":"fcysb3M7NPWt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-LgaR5INPWu"},"outputs":[],"source":["%tensorboard --logdir run/LinearRegression --port 6012"],"id":"s-LgaR5INPWu"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"01_PredictCarPrice.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}