{"cells":[{"cell_type":"markdown","source":["<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n","\n","> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n"," <br>\n","This program and the accompanying materials are made available under the\n","terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n","WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n","License for the specific language governing permissions and limitations\n","under the License. <br>\n","<br>**SPDX-License-Identifier: Apache-2.0** "],"metadata":{"id":"QZ8_rATdS42e"},"id":"QZ8_rATdS42e"},{"cell_type":"markdown","metadata":{"deletable":false,"id":"_zR_Hb6qU9XQ"},"source":["# Hyperparameter Tuning Exercise\n","Authored by : [Nazurah Kamil](mailto:nazurah.kamil@skymind.my)\n","\n","## Learning Outcome\n","By the end of this notebook, you will be able to:\n","- Perform hyperparameter tuning on relevant hyperparameters\n","- Avoid overfitting model\n","- Train this model to reach optimum accuracy\n","\n","\n","### **Dataset Used: Wine Classification Dataset**\n","This is a dataset with data on red and white variants of the Portuguese \"Vinho Verde\" wine. The dataset can be retrieved from [here](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009).<br>\n","\n","**Content :**<br>\n","For more information on this dataset, you may refer to <a href=https://repositorium.sdum.uminho.pt/bitstream/1822/10029/1/wine5.pdf>[Cortez et al., 2009]</a>.<br>\n","\n","**Input variables**<br>\n","These variables below are the features that are to be used in this modelling task:<br>\n","1 - fixed acidity<br>\n","2 - volatile acidity<br>\n","3 - citric acid<br>\n","4 - residual sugar<br>\n","5 - chlorides<br>\n","6 - free sulfur dioxide<br>\n","7 - total sulfur dioxide<br>\n","8 - density<br>\n","9 - pH<br>\n","10 - sulphates<br>\n","11 - alcohol<br>\n","\n","**Output variable (based on sensory data):**<br>\n","These variables below are the labels that are to be used in this modelling task:\n","12 - quality (score between 0 and 10)\n","\n","We will approach this task in the manner of a binary classification task even though there are a total of 6 classes for the target variable. In order to achieve this, we would first need to bin the target variable into 2 classes, which are:\n","- **0**: Lesser quality, consists of labels from 3 to 5 (inclusive)\n","- **1**: Better quality, consists of labels from 6 to 8 (inclusive)\n","\n","We would perform the preprocessing step as we go along."],"id":"_zR_Hb6qU9XQ"},{"cell_type":"markdown","metadata":{"id":"zFUhecJgU9XQ"},"source":["## Task :\n","For this noteboook, you are required to complete the following tasks:\n","\n","1. Build a deep neural network model which converged at the end\n","2. Prevent overfitting issue (difference between train and test accuracy falls within 3%)\n","3. Achieve a minimum of 75% accuracy for train and test set"],"id":"zFUhecJgU9XQ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yz2gD23U9XR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Torch libraries\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","# Path to find dataset\n","from pathlib import Path\n","\n","# Prevent warning \n","import warnings\n","warnings.filterwarnings('ignore')"],"id":"8yz2gD23U9XR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWQuGMbJU9XS"},"outputs":[],"source":["# Path specification\n","import pandas as pd\n","\n","\n","wine_data = pd.read_csv(\"https://raw.githubusercontent.com/Saranya-Skymind/Datasets/main/wine.csv\")"],"id":"fWQuGMbJU9XS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCP_v0A2U9XS"},"outputs":[],"source":["# Print the first 5 rows of our dataset\n","wine_data.head()"],"id":"VCP_v0A2U9XS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fH4V_4pfU9XT"},"outputs":[],"source":["wine_data.shape"],"id":"fH4V_4pfU9XT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VntQvEbrU9XU"},"outputs":[],"source":["wine_data.describe()"],"id":"VntQvEbrU9XU"},{"cell_type":"code","execution_count":null,"metadata":{"id":"63SuXR9KU9XU"},"outputs":[],"source":["# Check missing values\n","wine_data.info()"],"id":"63SuXR9KU9XU"},{"cell_type":"markdown","metadata":{"id":"NyjqAKi9U9XV"},"source":["Here, all the columns in the dataset are `int64` and `float64` data types. Next, let's display the clasess for the target variable."],"id":"NyjqAKi9U9XV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWJ6O6vYU9XV"},"outputs":[],"source":["wine_data['quality'].unique()"],"id":"hWJ6O6vYU9XV"},{"cell_type":"markdown","metadata":{"id":"E--Na1r5U9XW"},"source":["Let's bin our target variable to just two classes: **0** (lesser quality) and **1** (better quality)."],"id":"E--Na1r5U9XW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"x--EvgIwU9XW"},"outputs":[],"source":["# if wine quality is more than 6, label it as 1, otherwise 0\n","wine_data['quality'] = np.where(wine_data['quality'] >= 6, 1, 0)\n","\n","# Check again classes of target variable\n","wine_data['quality'].unique()"],"id":"x--EvgIwU9XW"},{"cell_type":"markdown","metadata":{"id":"J3C8Q3YEU9XW"},"source":["Next, let us check for the class distribution of target varable. "],"id":"J3C8Q3YEU9XW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8Y7vbuBU9XW"},"outputs":[],"source":["wine_data['quality'].value_counts()"],"id":"Z8Y7vbuBU9XW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MY6i7sUpU9XX"},"outputs":[],"source":["class_0_distribution = wine_data['quality'].value_counts()[0] / len(wine_data) * 100\n","class_1_distribution = wine_data['quality'].value_counts()[1] / len(wine_data) * 100\n","print(\"Class 0 is: \", class_0_distribution)\n","print(\"Class 1 is: \", class_1_distribution)"],"id":"MY6i7sUpU9XX"},{"cell_type":"markdown","metadata":{"id":"NYVS9XzQU9XX"},"source":["Here we can see that our data has 53% of 'better quality' wine and others is 'lesser quality' wine. Let's proceed with the next step which is to separate between features and label of our data."],"id":"NYVS9XzQU9XX"},{"cell_type":"markdown","metadata":{"id":"fHMOEso7U9XX"},"source":["## Features and Label\n","Here we will split the dataset into features and label."],"id":"fHMOEso7U9XX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDmGxV7dU9XX"},"outputs":[],"source":["X = wine_data.iloc[:, :-1].values\n","y = wine_data.iloc[:, -1].values"],"id":"fDmGxV7dU9XX"},{"cell_type":"markdown","metadata":{"id":"2T1PH1AYU9XX"},"source":["## Split into Train And Test\n","Split into train and test by using `train_test_split`."],"id":"2T1PH1AYU9XX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"n47ofQehU9XY"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 21)"],"id":"n47ofQehU9XY"},{"cell_type":"markdown","metadata":{"id":"9uz4ifCOU9XY"},"source":["## Data Preprocessing\n","Perform feature scaling onto `X_train` and `X_test`."],"id":"9uz4ifCOU9XY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLg6f6GfU9XY"},"outputs":[],"source":["# Feature scaling\n","scaler = MinMaxScaler()\n","\n","# Only fit the train_set but transform both train and test sets\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"id":"fLg6f6GfU9XY"},{"cell_type":"markdown","metadata":{"id":"cpjxnfBmU9XY"},"source":["## Hyperparameters\n","\n","Hyperparameter tuning is the process of selecting a set of ideal hyperparameters for a machine learning algorithm. A hyperparameter is a model argument whose value is set before start of the model training process.\n","\n","Here are the hyperparameters that can be used to fine tune our model."],"id":"cpjxnfBmU9XY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zZEeyj-U9XY"},"outputs":[],"source":["# Hyperparameter\n","num_input = 11\n","num_classes = 1\n","\"\"\"\n","Tweak this hyperparameter to increase accuracy\n","\"\"\"\n","epochs = 20\n","learning_rate = 0.001"],"id":"6zZEeyj-U9XY"},{"cell_type":"markdown","metadata":{"id":"DWXejkhBU9XZ"},"source":["## Dataset"],"id":"DWXejkhBU9XZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdNGU0HWU9XZ"},"outputs":[],"source":["# Create Custom_Dataset class\n","class Custom_Dataset(Dataset):\n","    def __init__(self, features, labels):\n","        # Convert numpy to tensor to tensor\n","        self.features = torch.tensor(features, dtype = torch.float32)\n","        self.labels = torch.tensor(labels, dtype  = torch.float32)\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        return self.features[idx], self.labels[idx]"],"id":"xdNGU0HWU9XZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGZfTTT9U9XZ"},"outputs":[],"source":["# Implement class Custom_Dataset\n","train_set = Custom_Dataset(X_train, y_train)\n","test_set = Custom_Dataset(X_test, y_test)"],"id":"GGZfTTT9U9XZ"},{"cell_type":"markdown","metadata":{"id":"RHRWvjQrU9XZ"},"source":["## DataLoader\n","`DataLoader` enables us to perform mini-batching and shuffling data using `Dataset` object which we have just instantiated just now."],"id":"RHRWvjQrU9XZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9M9eHINKU9XZ"},"outputs":[],"source":["# Input train_set and test_set into DataLoader\n","train_loader = DataLoader(train_set, batch_size = 128, shuffle = True)\n","test_loader = DataLoader(test_set, batch_size = 128, shuffle = False)\n","\n","dataloader = {'train':train_loader, 'test':test_loader}"],"id":"9M9eHINKU9XZ"},{"cell_type":"markdown","metadata":{"id":"cpJJm0rYU9XZ"},"source":["# Model Training\n","For this training, we will be using `nn.Module` because it has more flexibility in designing the network. For example, you can write your own `forward()` method."],"id":"cpJJm0rYU9XZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEV982QJU9XZ"},"outputs":[],"source":["# Create class for LogisticRegression \n","class LogisticRegression(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc_1 = nn.Linear(num_input, 500)\n","        \"\"\"\n","        Increase/Adding layer until error does not improve anymore\n","        \"\"\"\n","        self.fc_2 = nn.Linear(500, 150)\n","        self.fc_3 = nn.Linear(150, num_classes)\n","        \n","    def forward(self, x):\n","        y_hat = F.relu(self.fc_1(x))\n","        y_hat = F.relu(self.fc_2(y_hat))\n","        # add activation function at output layer - sigmoid\n","        y_hat = F.sigmoid(self.fc_3(y_hat))\n","        return y_hat"],"id":"dEV982QJU9XZ"},{"cell_type":"markdown","metadata":{"id":"7DOKb0bKU9Xa"},"source":["###### Change optimizer and criterion accordingly"],"id":"7DOKb0bKU9Xa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFbkl4YzU9Xa"},"outputs":[],"source":["torch.manual_seed(5)\n","model = LogisticRegression()\n","\"\"\"\n","Change any optimizer and criterion that match\n","\"\"\"\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","criterion = nn.BCELoss()"],"id":"xFbkl4YzU9Xa"},{"cell_type":"markdown","metadata":{"id":"DYVLM_4xU9Xa"},"source":["By default, our model is in training mode `model.train`. For model validation and testing, we will set model to evaluation mode with `model.eval`. \n","\n","* **model.train**<br>\n","It informs our model that we are in the training phase, so it keeps all layers active, such as dropout and batch-normalization, which usage is to prevent overfitting of model during training phase.\n","\n","* **model.eval**<br>\n","`model.eval` does the opposite. As a result, after using `model.eval`, our model deactivates such layers (whose sole function is to prevent overfitting during training phase), allowing the model to output its inference as predicted."],"id":"DYVLM_4xU9Xa"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"M7HD-1_DU9Xa"},"outputs":[],"source":["\n","# Keep track of Loss and accuracy\n","loss_score = {'train': [], 'test': []}\n","accuracy_score = {'train': [], 'test': []}\n","\n","for epoch in range(1, epochs+1):\n","    print(f'\\nEpoch {epoch}\\n--------')\n","    \n","    for loader in ['train', 'test']:\n","        running_loss = 0.0\n","        running_size = 0\n","        correct = 0\n","        \n","        if loader == 'train':\n","            model.train()\n","        else:\n","            model.eval()\n","\n","        for X, y in dataloader[loader]:\n","            with torch.set_grad_enabled(loader == 'train'):\n","                output = model(X)\n","                loss = criterion(output, y.unsqueeze(1)) # Unsqueeze to prevent broadcasting\n","                \n","                # Calculate loss\n","                running_loss += loss.item()*output.size(0)\n","                running_size += output.size(0) # To get until remainder batch\n","\n","                # Calculate accuracy\n","                # Set threshold for sigmoid instead of using torch.max\n","                predictions = output > 0.5\n","                correct += (predictions == y.unsqueeze(1)).sum().item()\n","\n","                if loader == 'train':\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","\n","        # Accuracy and loss per epoch\n","        accuracy = (100*correct) / running_size\n","        loss_per_epoch = running_loss / running_size\n","        \n","        print(f'{loader.capitalize()} Loss:{loss_per_epoch} {loader.capitalize()} Accuracy:{accuracy}')\n","        loss_score[loader].append(loss_per_epoch)\n","        accuracy_score[loader].append(accuracy)"],"id":"M7HD-1_DU9Xa"},{"cell_type":"markdown","metadata":{"id":"o067b78UU9Xa"},"source":["The model training has completed, we can now visualize the loss of model by epoch."],"id":"o067b78UU9Xa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtJdKmt7U9Xa"},"outputs":[],"source":["# Visualize loss\n","fig, ax = plt.subplots()\n","fig.set_size_inches(14, 7)\n","ax.set_title(\"Loss Score against Epoch\")\n","ax.grid(b=True)\n","ax.set_xlabel(\"Epoch Number\")\n","ax.set_ylabel(\"Loss Score\")\n","\n","ax.plot(loss_score['train'], color='blue', label='Training Loss')\n","ax.plot(loss_score['test'], color='red', label='Testing Loss')\n","ax.legend();"],"id":"BtJdKmt7U9Xa"},{"cell_type":"markdown","metadata":{"id":"SCwlbihEU9Xb"},"source":["Let's also visualize the accuracy of model by epoch."],"id":"SCwlbihEU9Xb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkwMi8NiU9Xb"},"outputs":[],"source":["# Visualize accuracy\n","fig, ax = plt.subplots()\n","fig.set_size_inches(14, 7)\n","ax.set_title(\"Accuracy against Epoch\")\n","ax.grid(b=True)\n","ax.set_xlabel(\"Epoch Number\")\n","ax.set_ylabel(\"Accuracy\")\n","\n","ax.plot(accuracy_score['train'], color='blue', label='Training Accuracy')\n","ax.plot(accuracy_score['test'], color='red', label='Testing Accuracy')\n","ax.legend();"],"id":"FkwMi8NiU9Xb"}],"metadata":{"_draft":{"nbviewer_url":"https://gist.github.com/16aa2aa60e247ff5f22d8b0febe0e267"},"gist":{"data":{"description":"deep_learning/Underfit Model.ipynb","public":true},"id":"16aa2aa60e247ff5f22d8b0febe0e267"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"01_MLP_HyperparameterTuning.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}