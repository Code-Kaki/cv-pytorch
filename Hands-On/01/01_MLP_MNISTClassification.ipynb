{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "QZ8_rATdS42e",
   "metadata": {
    "id": "QZ8_rATdS42e"
   },
   "source": [
    "<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n",
    "\n",
    "> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program and the accompanying materials are made available under the\n",
    "terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "License for the specific language governing permissions and limitations\n",
    "under the License. <br>\n",
    "<br>**SPDX-License-Identifier: Apache-2.0** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y7AaP40XxLH5",
   "metadata": {
    "id": "Y7AaP40XxLH5"
   },
   "source": [
    "# MNIST - MultiLayer Perceptron\n",
    "Authored by : [Nazurah Kamil](mailto:nazurah.kamil@skymind.my)\n",
    "\n",
    "In this notebook, we will be applying <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>multilayer perceptron</a>\n",
    "algorithm to classify number from 0 - 9 by using only linear layers. <a href=https://en.wikipedia.org/wiki/MNIST_database>MNIST\n",
    "dataset</a> will be implemented throughout this hands-on. <br>\n",
    "\n",
    "Let's import the library that we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k9M2V72ExLH5",
   "metadata": {
    "id": "k9M2V72ExLH5"
   },
   "source": [
    "At the end of the notebook you will be able to:\n",
    "* Know how to use the MNIST dataset.\n",
    "* Classify the numbers in the MNIST dataset.\n",
    "* Understand how to work with the multilayer perceptron that use linear layers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i5Zo7S8ZxLH6",
   "metadata": {
    "id": "i5Zo7S8ZxLH6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OXXKeXA8xLH7",
   "metadata": {
    "id": "OXXKeXA8xLH7"
   },
   "source": [
    "The first step is to download and perform transformation to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eAtA9dprxLH8",
   "metadata": {
    "id": "eAtA9dprxLH8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downloading and loading data... download may take some time\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root='../datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root='../datasets',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DXoOHgywxLH9",
   "metadata": {
    "id": "DXoOHgywxLH9"
   },
   "outputs": [],
   "source": [
    "# Show the first result of training set\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_mgVJqB5xLH9",
   "metadata": {
    "id": "_mgVJqB5xLH9"
   },
   "source": [
    "From the first record, it returns a two-item tuple. The first item of our data is an image, and the second item is a label. In our dataset, it shows the first label is 5. Next, let see the size of our first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jfa_h8sgxLH-",
   "metadata": {
    "id": "Jfa_h8sgxLH-"
   },
   "outputs": [],
   "source": [
    "# Show the size and label for our first record data\n",
    "image, label = train_set[0]\n",
    "print(\"Shape : {}, Label : {}\".format(image.shape, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uqZrIgaExLH-",
   "metadata": {
    "id": "uqZrIgaExLH-"
   },
   "outputs": [],
   "source": [
    "# Show the first image of trainset\n",
    "plt.imshow(train_set[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uuuU4X1jxLH_",
   "metadata": {
    "id": "uuuU4X1jxLH_"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zBYXgml-xLH_",
   "metadata": {
    "id": "zBYXgml-xLH_"
   },
   "source": [
    "Use `DataLoader` to load the dataset, so that the dataset is iterable. Batch size is configured here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H-OvpB0lxLH_",
   "metadata": {
    "id": "H-OvpB0lxLH_"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)  # For consistent results\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "# Put loader into dictionary\n",
    "dataloaders = {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v5x3NLYQxLH_",
   "metadata": {
    "id": "v5x3NLYQxLH_"
   },
   "source": [
    "It's important to have a balanced dataset to prevent overfitting during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vQnZOhQ3xLIA",
   "metadata": {
    "id": "vQnZOhQ3xLIA"
   },
   "outputs": [],
   "source": [
    "# Count each label\n",
    "total = 0\n",
    "count_dict = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
    "\n",
    "for data in train_loader:\n",
    "    image, label = data\n",
    "    for y in label:\n",
    "        count_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AiXP_soyxLIA",
   "metadata": {
    "id": "AiXP_soyxLIA"
   },
   "outputs": [],
   "source": [
    "# Percentage of each label\n",
    "for i in count_dict:\n",
    "    print(\"{} : {:.2f} % \".format(i, count_dict[i]/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2LSo486SxLIA",
   "metadata": {
    "id": "2LSo486SxLIA"
   },
   "source": [
    "It seems that our dataset is quite balanced. Next, let's view the first 10 images from `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yy3Bk3lbxLIA",
   "metadata": {
    "id": "yy3Bk3lbxLIA"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'int': (lambda x: f'{x:4}')})  # Widen the array\n",
    "\n",
    "# Grab the first batch of image\n",
    "for images, labels in train_loader:\n",
    "    break\n",
    "\n",
    "# Print the first 10 labels\n",
    "print('Labels: ', labels[:10].numpy())\n",
    "\n",
    "# Print the first 10 images\n",
    "im = make_grid(images[:10], nrow=10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(np.transpose(im, (1, 2, 0)))  # tranpose from CHW to WHC\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1pkGvJ6OxLIB",
   "metadata": {
    "id": "1pkGvJ6OxLIB"
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9GWESoDyxLIB",
   "metadata": {
    "id": "9GWESoDyxLIB"
   },
   "source": [
    "**Set Up Hyperparameter**<br>\n",
    "Here, we know that our images are in 28x28 size. So, we need to flatten it into 784 (28x28) to fit into the model. This\n",
    "784 will be the number of inputs in the model. The number of outputs is 10 because we want to classify 10 labels\n",
    "(0, 1, 2, 3, 4, 5, 6, 7, 8, 9). <br>\n",
    "Other than that, hyperparameters required are the number of epoch and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-QloBL2bxLIB",
   "metadata": {
    "id": "-QloBL2bxLIB"
   },
   "outputs": [],
   "source": [
    "# Set up hyperparameter\n",
    "epochs = 10\n",
    "num_input = 784  # 28x28\n",
    "num_output = 10\n",
    "lr_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_SgDXh7OxLIB",
   "metadata": {
    "id": "_SgDXh7OxLIB"
   },
   "source": [
    "We will be using log_softmax as the activation function in the output layer. This part is **skipped** because we will be\n",
    "using Cross-Entropy Loss as our loss function. As stated in <a href=https://pytorch.org/docs/stable/nn.html#crossentropyloss>\n",
    "`torch.nn.CrossEntropyLoss()`</a> documentation. This ***criterion*** will combine `nn.LogSoftmax()` and `nn.NLLLoss()`\n",
    "in one single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m7IM0xmAxLIC",
   "metadata": {
    "id": "m7IM0xmAxLIC"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_input, 164)\n",
    "        self.fc2 = nn.Linear(164, 100)\n",
    "        self.fc3 = nn.Linear(100, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # Apply log_softmax in output layer (skip this part)\n",
    "        # return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wcXsLSIYxLIC",
   "metadata": {
    "id": "wcXsLSIYxLIC"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MultiLayerPerceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6qzJXjvMxLIC",
   "metadata": {
    "id": "6qzJXjvMxLIC"
   },
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QGHPBJkAxLIC",
   "metadata": {
    "id": "QGHPBJkAxLIC"
   },
   "source": [
    "These few layers have been implemented into our model :\n",
    "1. nn.Linear(784,164)\n",
    "2. nn.ReLU()\n",
    "3. nn.Linear(164,100)\n",
    "4. nn.ReLU()\n",
    "5. nn.Linear(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-A9mSCoExLIC",
   "metadata": {
    "id": "-A9mSCoExLIC"
   },
   "outputs": [],
   "source": [
    "# Calculate total model parameters\n",
    "sum = 0\n",
    "for param in model.parameters():\n",
    "    item = param.numel()\n",
    "    print(f'{item:>6}')\n",
    "    sum = sum + item\n",
    "print(f'-------\\n{sum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iSvWMCiuxLIC",
   "metadata": {
    "id": "iSvWMCiuxLIC"
   },
   "source": [
    "The sum of the model parameters is **146250**. Here you can see that we use a **large number of parameters** while in CNN,\n",
    "fewer parameters will be used to reduce computation power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GeJQbev2xLID",
   "metadata": {
    "id": "GeJQbev2xLID"
   },
   "source": [
    "# Flatten Image\n",
    "Let's see the batch tensor of our training data. This batch tensor have a batch of [100,1,28,28]. In order to apply our\n",
    "model to data, use `.view()` to flatten the size into [100, 784]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7XV3XDrxLID",
   "metadata": {
    "id": "g7XV3XDrxLID",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load first batch and print shape\n",
    "for images, labels in train_loader:\n",
    "    print('Batch shape (before flatten):', images.size(), labels.shape)\n",
    "    break\n",
    "\n",
    "# Flatten image\n",
    "print('After flatten image :', images.view(-1, 784).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p-8V7YZIxLID",
   "metadata": {
    "id": "p-8V7YZIxLID"
   },
   "source": [
    "Take note that we need to perform **flatten image** before input the data into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geKZ1fkxxLID",
   "metadata": {
    "id": "geKZ1fkxxLID"
   },
   "source": [
    "# Start the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3zjp63o0xLID",
   "metadata": {
    "id": "3zjp63o0xLID"
   },
   "source": [
    "Set up loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5mKlsSJxLID",
   "metadata": {
    "id": "s5mKlsSJxLID"
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4Dk1WMPxLIE",
   "metadata": {
    "id": "p4Dk1WMPxLIE"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('../run/MLP_MNIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpkkAS3TxLIE",
   "metadata": {
    "id": "tpkkAS3TxLIE"
   },
   "source": [
    "During the training, test score will be calculated for each epoch so that the loss graph for both train and test data\n",
    "are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ei4Em4dexLIE",
   "metadata": {
    "id": "Ei4Em4dexLIE"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "# Implement model training and validation loop\n",
    "loss_score = {'train': [], 'test': []}\n",
    "accuracy_score = {'train': [], 'test': []}\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'Epoch {epoch}\\n--------')\n",
    "\n",
    "    for loader in ['train', 'test']:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_size = 0\n",
    "        correct = 0\n",
    "        log_interval = 100\n",
    "\n",
    "        if loader == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iter, (X, y) in enumerate(dataloaders[loader]):\n",
    "            iter += 1\n",
    "            # Set gradient calculation on / off\n",
    "            with torch.set_grad_enabled(loader == 'train'):\n",
    "                output = model(X.view(-1, 784))  # Flatten the image here\n",
    "                loss = criterion(output, y)\n",
    "\n",
    "                # Calculate loss\n",
    "                running_loss += loss.item() * output.size(0)\n",
    "                running_size += output.size(0)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predict = torch.max(output, 1)[1]\n",
    "                correct += (predict == y).sum().item()\n",
    "\n",
    "                if loader == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # Print every 100 iteration\n",
    "                    if iter == 1 or (iter % log_interval) == 0:\n",
    "                        print('Iteration:{} Loss:{:.6} Accuracy:{:.6} Batch size:[{}/{}]'.format(\n",
    "                            int(iter),\n",
    "                            running_loss/running_size,\n",
    "                            (100*correct)/running_size,\n",
    "                            running_size,\n",
    "                            len(train_set)\n",
    "                        ))\n",
    "\n",
    "        # Accuracy and loss per epoch\n",
    "        accuracy = (100*correct) / running_size\n",
    "        loss_per_epoch = running_loss / running_size\n",
    "\n",
    "        print('\\n{} Loss:{}'.format(loader.capitalize(), loss_per_epoch))\n",
    "        print('{} Accuracy:{}'.format(loader.capitalize(), accuracy))\n",
    "\n",
    "        loss_score[loader].append(loss_per_epoch)\n",
    "        accuracy_score[loader].append(accuracy)\n",
    "\n",
    "        writer.add_scalars('Losses', {loader: loss_per_epoch}, epoch)\n",
    "        writer.add_scalars('Accuracy', {loader: accuracy}, epoch)\n",
    "    print('***************\\n')\n",
    "\n",
    "# Print the time elapsed\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7kSUHuBwxLIE",
   "metadata": {
    "id": "7kSUHuBwxLIE"
   },
   "outputs": [],
   "source": [
    "# Visualize loss\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 7)\n",
    "ax.set_title(\"Loss Score against Epoch\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Epoch Number\")\n",
    "ax.set_ylabel(\"Loss Score\")\n",
    "\n",
    "ax.plot(loss_score['train'], color='goldenrod', label='Training Loss')\n",
    "ax.plot(loss_score['test'], color='green', label='Test Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eQz6e5qwxLIE",
   "metadata": {
    "id": "eQz6e5qwxLIE"
   },
   "outputs": [],
   "source": [
    "# Visualize accuracy\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 7)\n",
    "ax.set_title(\"Accuracy against Epoch\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Epoch Number\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(accuracy_score['train'], color='goldenrod', label='Training Accuracy')\n",
    "ax.plot(accuracy_score['test'], color='green', label='Test Accuracy')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JknH6A-gxLIF",
   "metadata": {
    "id": "JknH6A-gxLIF"
   },
   "source": [
    "From the graph above, we can see that **this model overfitting the training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYyV5g2ExLIF",
   "metadata": {
    "id": "MYyV5g2ExLIF"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../run/MLP_MNIST --port 6009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VWLLSmWcxLIF",
   "metadata": {
    "id": "VWLLSmWcxLIF"
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pXZ3AOHxLIF",
   "metadata": {
    "id": "3pXZ3AOHxLIF"
   },
   "source": [
    "This step will save the learned parameter for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QgKdvXzfxLIF",
   "metadata": {
    "id": "QgKdvXzfxLIF"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"../generated_model\"): os.makedirs(\"../generated_model\") # Create folder\n",
    "torch.save(model.state_dict(), '../generated_model/mlp_MNIST.pt')  # Save the learned parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FLTyYyKrxLIF",
   "metadata": {
    "id": "FLTyYyKrxLIF"
   },
   "source": [
    "# Load Model\n",
    "\n",
    "Here, we want to test the model object to our test set to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xz7r48CCxLIF",
   "metadata": {
    "id": "xz7r48CCxLIF"
   },
   "outputs": [],
   "source": [
    "new_model = MultiLayerPerceptron()\n",
    "new_model.load_state_dict(torch.load('../generated_model/mlp_MNIST.pt'))\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VRq8mxK5xLIF",
   "metadata": {
    "id": "VRq8mxK5xLIF"
   },
   "outputs": [],
   "source": [
    "correct = 0.0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        output = new_model(X.view(-1, 784))\n",
    "        # Loss\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        # Accuracy\n",
    "        predict = torch.max(output, 1)[1]\n",
    "        correct += (predict == y).sum()\n",
    "\n",
    "    correct = correct / len(test_loader.dataset)\n",
    "print(f'Loss : {loss:.4f} Accuracy :{correct*100:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P3UBeEqHxLIG",
   "metadata": {
    "id": "P3UBeEqHxLIG"
   },
   "source": [
    "# Test on a Single Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FJvEPNUfxLIG",
   "metadata": {
    "id": "FJvEPNUfxLIG"
   },
   "source": [
    "We want to test if our model can predict the first data point of test set. Below shows the actual label for the first\n",
    "data point of test set is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DS20suMtxLIG",
   "metadata": {
    "id": "DS20suMtxLIG"
   },
   "outputs": [],
   "source": [
    "image, label = test_set[0]\n",
    "print(\"Shape : {}, Label : {}\".format(image.shape, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PMCJwO8lxLIG",
   "metadata": {
    "id": "PMCJwO8lxLIG"
   },
   "outputs": [],
   "source": [
    "for image, label in test_loader:\n",
    "\n",
    "    output = new_model(image.view(-1, 784))\n",
    "    predict = torch.max(output, 1)[1]\n",
    "\n",
    "    print(f'Prediction: {predict[0]}')\n",
    "\n",
    "    print(f'Label size: {label.size()}')\n",
    "\n",
    "    print(f'Actual label: {label[0]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nfHUH-ySxLIG",
   "metadata": {
    "id": "nfHUH-ySxLIG"
   },
   "source": [
    "Looks like our model predicted correctly. Next, we will be implemented gpu for faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qrl-P8OyxLIG",
   "metadata": {
    "id": "Qrl-P8OyxLIG"
   },
   "source": [
    "# (Optional) - Using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_sxcyrjSxLIG",
   "metadata": {
    "id": "_sxcyrjSxLIG"
   },
   "source": [
    "***When do we need GPU?***<br>\n",
    "These are the common situation that we might face during training deep learning model:\n",
    "* Large dataset\n",
    "* Large model that contains up to million parameters\n",
    "* Complex model<br>\n",
    "\n",
    "All of the situations above might slow down training the deep learning model. So, we need to leverage GPU to speed up the training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CpmPRLtGxLIH",
   "metadata": {
    "id": "CpmPRLtGxLIH"
   },
   "source": [
    "***Important Note :***<br><br>\n",
    "You will need to install cudatoolkit and compatible cudnn within your laptop.\n",
    "In this course, we will be using cuda version 10.2 and cudnn version 7.6.5.\n",
    "Follow this <a href=https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows>link</a> if you want to install cuda on your laptop. \n",
    "\n",
    " ***Below steps are skipped because we have installed cuda on our environment***\n",
    "* **cudatoolkit** - Please download pytorch version that includes \"cudatoolkit\" at <a href=https://pytorch.org/get-started/locally/>pytorch.org</a>.<br>\n",
    "* **cudnn version** - Next step is to find cudnn version that is compatible with your laptop/computer.\n",
    "Please refer <a href=https://www.programmersought.com/article/20033556147/>here</a> for more info.<br>\n",
    "* **cuda-pytorch** - If you want to know more about cuda in pytorch, click <a href=https://pytorch.org/docs/stable/notes/cuda.html>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9OdmX_cgxLIH",
   "metadata": {
    "id": "9OdmX_cgxLIH"
   },
   "source": [
    "First, we want to know if we have CUDA available on our laptop or desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opqFgcjDxLIH",
   "metadata": {
    "id": "opqFgcjDxLIH"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IhVzq4-sxLIH",
   "metadata": {
    "id": "IhVzq4-sxLIH"
   },
   "source": [
    "There are two things required to be GPU compatible:\n",
    "1. Model\n",
    "2. Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IxL6la78xLIH",
   "metadata": {
    "id": "IxL6la78xLIH"
   },
   "outputs": [],
   "source": [
    "# Get Id of default device\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hBxfDhuhxLIH",
   "metadata": {
    "id": "hBxfDhuhxLIH"
   },
   "outputs": [],
   "source": [
    "# Input our model in gpu\n",
    "model_gpu = MultiLayerPerceptron()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model_gpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_BstuKHJxLIH",
   "metadata": {
    "id": "_BstuKHJxLIH"
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_gpu.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fY-9a8UwxLII",
   "metadata": {
    "id": "fY-9a8UwxLII"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "# Implement model training and validation loop\n",
    "loss_score = {'train': [], 'test': []}\n",
    "accuracy_score = {'train': [], 'test': []}\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'Epoch {epoch}\\n--------')\n",
    "\n",
    "    for loader in ['train', 'test']:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_size = 0\n",
    "        correct = 0\n",
    "        log_interval = 100\n",
    "\n",
    "        if loader == 'train':\n",
    "            model_gpu.train()\n",
    "        else:\n",
    "            model_gpu.eval()\n",
    "\n",
    "        for iter, (X, y) in enumerate(dataloaders[loader]):\n",
    "\n",
    "            \"\"\"Use GPU For Model \"\"\"\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "            # Set gradient calculation on / off\n",
    "            with torch.set_grad_enabled(loader == 'train'):\n",
    "                output = model_gpu(X.view(-1, 784))  # Flatten the image here\n",
    "                loss = criterion(output, y)\n",
    "\n",
    "                # Calculate loss\n",
    "                running_loss += loss.item() * output.size(0)\n",
    "                running_size += output.size(0)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predict = torch.max(output, 1)[1]\n",
    "                correct += (predict == y).sum().item()\n",
    "\n",
    "                if loader == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # Print every 100 iteration\n",
    "                    if iter == 1 or (iter % log_interval) == 0:\n",
    "                        print('Iteration:{} Loss:{:.6} Accuracy:{:.6} Batch size:[{}/{}]'.format(\n",
    "                            int(iter),\n",
    "                            running_loss/running_size,\n",
    "                            (100*correct)/running_size,\n",
    "                            running_size,\n",
    "                            len(train_set)\n",
    "                        ))\n",
    "\n",
    "        # Accuracy and loss per epoch\n",
    "        accuracy = (100*correct) / running_size\n",
    "        loss_per_epoch = running_loss / running_size\n",
    "\n",
    "        print('\\n{} Loss:{}'.format(loader.capitalize(), loss_per_epoch))\n",
    "        print('{} Accuracy:{}'.format(loader.capitalize(), accuracy))\n",
    "\n",
    "        loss_score[loader].append(loss_per_epoch)\n",
    "        accuracy_score[loader].append(accuracy)\n",
    "\n",
    "    print('***************\\n')\n",
    "\n",
    "# Print the time elapsed\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35UkYFQXxLII",
   "metadata": {
    "id": "35UkYFQXxLII"
   },
   "outputs": [],
   "source": [
    "# Visualize loss\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 7)\n",
    "ax.set_title(\"Loss Score against Epoch\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Epoch Number\")\n",
    "ax.set_ylabel(\"Loss Score\")\n",
    "\n",
    "ax.plot(loss_score['train'], color='goldenrod', label='Training Loss')\n",
    "ax.plot(loss_score['test'], color='green', label='Test Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1p31D65XxLII",
   "metadata": {
    "id": "1p31D65XxLII"
   },
   "outputs": [],
   "source": [
    "# Visualize accuracy\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(14, 7)\n",
    "ax.set_title(\"Accuracy against Epoch\")\n",
    "ax.grid(b=True)\n",
    "ax.set_xlabel(\"Epoch Number\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(accuracy_score['train'], color='goldenrod', label='Training Accuracy')\n",
    "ax.plot(accuracy_score['test'], color='green', label='Test Accuracy')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mf0wKbHqxLII",
   "metadata": {
    "id": "mf0wKbHqxLII"
   },
   "source": [
    "It took a shorter time to train the model if we are using GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maY8Zv46xLII",
   "metadata": {
    "id": "maY8Zv46xLII"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Multilayer perceptron might cause our model to overfit the training data. Furthermore, full connectivity is wasteful\n",
    "and used a huge number of parameters to train the model. This will take a longer time to train the model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6qzJXjvMxLIC",
    "GeJQbev2xLID",
    "VWLLSmWcxLIF",
    "FLTyYyKrxLIF",
    "P3UBeEqHxLIG",
    "Qrl-P8OyxLIG",
    "maY8Zv46xLII"
   ],
   "name": "01_MLP_MNISTClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
