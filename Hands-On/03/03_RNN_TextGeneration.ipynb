{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_RNN_TextGeneration.ipynb","provenance":[],"collapsed_sections":["dVYGFpxC6zNJ"],"private_outputs":true,"authorship_tag":"ABX9TyM/C3viIm9v0BTLPEKq16jl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n","\n","> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n"," <br>\n","This program and the accompanying materials are made available under the\n","terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n","WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n","License for the specific language governing permissions and limitations\n","under the License. <br>\n","<br>**SPDX-License-Identifier: Apache-2.0** "],"metadata":{"id":"QZ8_rATdS42e"}},{"cell_type":"markdown","source":["# Recurrent Neural Network: Text Generation \n"],"metadata":{"id":"85zQOHOkUAtN"}},{"cell_type":"markdown","source":["## Recalling RNN\n","\n","Traditional feed-forward neural networks take in a fixed amount of input data all at the same time and produce a fixed amount of output each time. On the other hand, RNNs do not consume all the input data at once. Instead, they take them in one at a time and in a sequence. At each step, the RNN does a series of calculations before producing an output. The output, known as the hidden state, is then combined with the next input in the sequence to produce another output. This process continues until the model is programmed to finish or the input sequence ends.\n","\n","You might be wondering, which portion of the RNN do I extract my output from? This really depends on what your use case is. For example, if you’re using the RNN for a classification task, you’ll only need one final output after passing in all the input - a vector representing the class probability scores. In another case, if you’re doing text generation based on the previous character/word, you’ll need an output at every single time step.\n","![image](https://user-images.githubusercontent.com/79887667/121659305-c14f2880-cad4-11eb-94cc-ccbaa4aa0dc9.png)\n","\n","For Time Series :\n","* Forecasting - many-to-many or many-to-one\n","* Classification - many-to-one\n","\n","For NLP :\n","* Text Classification: many-to-one\n","* Text Generation: many-to-many\n","* Machine Translation: many-to-many\n","* Named Entity Recognition: many-to-many\n","* Image Captioning: one-to-many"],"metadata":{"id":"qp9ga9ovCpbM"}},{"cell_type":"markdown","source":["## Introduction\n","\n","In this hands-on, we will be building a simple text generator based on an RNN model, something like the ones that is used in autocompleters. \n","\n","During inferencing, the model will be fed with a word or a sequence of starting characters. The output of the model will be a prediction of the next character in the sentence. This process will repeat itself until we generate a sentence of our desired length.\n","\n","To keep things short and simple, we won't be using any large, external dataset. Instead, we will provide a few sentences and dive into how the model learns from these sentences.\n","\n","Let's get started!"],"metadata":{"id":"ZesV0tKuQvIR"}},{"cell_type":"markdown","source":["## Objectives\n","In this hands-on, we will :-\n","\n","1. Create sample sentences to train our model with.\n","2. Ensure that the sentences are all of the same length.\n","3. Encode the string to numerical characters.\n","4. Instantiate the Model class.\n","5. Instantiate the optimizer and loss function.\n","6. Set the training hyperparameters and train the model.\n","7. Test the model"],"metadata":{"id":"G4M1ZnIUSa3i"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.nn import functional as F\n","\n","# Seeding for reproducibility\n","torch.manual_seed(3)"],"metadata":{"id":"nUEjTaZLCwG7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's create a list of sentences to train our model with. Here there are 6 sample sentences."],"metadata":{"id":"yLfMF51Ic25U"}},{"cell_type":"code","source":["text = [\n","        'hey how are you', \n","        'good i am fine', \n","        'have a nice day',\n","        'you are pretty', \n","        'i am awesome', \n","        'good luck'\n","        ]"],"metadata":{"id":"hNqMutnac12e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll be padding our input sentences to ensure that all the sentences are of standard length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in mini-batches to optimize the training process. In order to use batches to train on our data, we'll need to ensure that each sequence within the input data is of equal size.\n","\n","Therefore, in most cases, padding can be done by filling up sequences that are too short with 0 values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."],"metadata":{"id":"qdtUYVhCXVFC"}},{"cell_type":"code","source":["# Finding the length of the longest string in our data\n","max_len = len(max(text, key=len))\n","print(f\"max_len={max_len}\")\n","\n","# append each sentence with a space (' ') until they all are in the same length\n","for i,_ in enumerate(text):\n","  while len(text[i]) < max_len:\n","    text[i] += ' ' "],"metadata":{"id":"xGozlp9zXTRF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similar to training all the other model prior to this, we need an array of labelled data to train our model. \n","\n","Since in this project we will be predicting the upcoming character to an input character, we'll divide each sentence (Eg: \"hey how are you\") into:\n","\n","  **1. Input Data:**\n","  - Strip the last character of the sentence\n","  - Eg: \"hey how are yo\" <br>\n","\n","  \n","  **2. Target:**\n","  - Strip the first character of the sentence\n","  - Eg: \"ey how are you\"\n","\n","**How is this going to work?**\n","1. Using the above example sentence, the first character of the input data will be 'h'. The model will be trained to predict the first target, 'e'. This is followed by:\n","  - input ('e'), target ('y') > \n","  - input ('y'), target (' ') >\n","  - input (' '), target ('h') and on...\n","\n","2. The idea is to have the target to be a character that is one time-step ahead of the input character.\n","\n","_SIDE NOTE: Since the sequence of the output characters is crucial to ensure that the output sentence is English and not gibberish, we must be careful to not misarrange our training set._"],"metadata":{"id":"3PbnaIHda0ao"}},{"cell_type":"code","source":["# building input and target sequence\n","input_seq = []\n","target_seq = []\n","\n","for i,_ in enumerate(text):\n","  # Remove last character for input sequence\n","  input_seq.append(text[i][:-1])\n","\n","  # Remove first character for target sequence\n","  target_seq.append(text[i][1:])\n","\n","  print(f\"Input Sequence: {input_seq[i]}\", end=\"\\n\")\n","  print(f\"Target Sequence: {target_seq[i]}\\n\")"],"metadata":{"id":"874afzV0aaEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gE0YlaeO6zNE"},"source":["## Encoding"]},{"cell_type":"markdown","metadata":{"id":"NA8-VcNk6zNE"},"source":["Now we can convert our input and target sequences to sequences of integers instead of a sequence of characters by mapping them using the dictionaries we created above. Subsequently, this will allow us to one-hot-encode our input sequence. In other words, we need to perform **integer encoding** on our data before being able to perform **one-hot encoding**. \n","\n","Although there are classes available in the `scikit-learn` library (Eg: `sklearn.preprocessing.LabelEncoder`, `sklearn.preprocessing.OneHotEncoder`), we will rather do them manually to breakdown what is going on during an encoding. You may try to recreate the encodings using these classes in your own time."]},{"cell_type":"code","source":["# Join all sentences together and extract the unique characters from the combined sentences\n","unique_chars = set(''.join(text))\n","\n","# Create a dictionary that maps characters to integers (for encoding)\n","char2int = dict((c,i) for i,c in enumerate(unique_chars))\n","\n","# Create a dictionary that maps integers to characters (for decoding)\n","int2char = dict((i,c) for i,c in enumerate(unique_chars))"],"metadata":{"id":"SNaA8bJRTLvf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `char2int` dictionary holds all the unique letters/symbols that were present in our sentences and maps each of them to a unique integer."],"metadata":{"id":"FMn0iMjkXMQA"}},{"cell_type":"code","source":["print(f\"Length of 'char2int' is {len(char2int)}\")\n","print(char2int)"],"metadata":{"id":"yeuaEGhNXLoT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`int2char` is a reverse mapping of `char2int`."],"metadata":{"id":"9JcDhZDzkw9u"}},{"cell_type":"code","source":["print(f\"Length of 'int2char' is {len(int2char)}\")\n","print(int2char)"],"metadata":{"id":"X9ETxmtoyXB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Integer Encoding"],"metadata":{"id":"s7Fzu4_wlukN"}},{"cell_type":"code","source":["def integer_encoding(seq_list:list, mapping_dict:dict):\n","  \"\"\"Encodes a list of \"\"\"\n","  encoded_data = []\n","  for i,_ in enumerate(seq_list):\n","    encoded_data.append([mapping_dict[character] for character in seq_list[i]])\n","  return encoded_data"],"metadata":{"id":"FehOJaEIvwAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Before integer encoding: {input_seq[0]}\")\n","\n","input_seq_int_enc = integer_encoding(input_seq, char2int)\n","target_seq_int_enc = integer_encoding(target_seq, char2int)\n","\n","print(f\"After integer encoding: {input_seq_int_enc[0]}\")"],"metadata":{"id":"ZrPbRFQkmFfR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### One-Hot Encoding"],"metadata":{"id":"BYiyIxuln5v3"}},{"cell_type":"code","source":["def one_hot_encoding(integer_encoded_data, mapping_dict : dict):\n","  # initialize the output with zeros\n","  total_seqs = len(integer_encoded_data)\n","  seq_len = len(integer_encoded_data[0])\n","  encoding_vector = len(mapping_dict)\n","\n","  one_hot_encoded = np.zeros((total_seqs, seq_len, encoding_vector), dtype=np.float32) # Shape: (6,14,22)\n","\n","  for i, seq in enumerate(integer_encoded_data):\n","    for j, integer in enumerate(seq):\n","      one_hot_encoded[i, j, integer] = 1\n","  return one_hot_encoded"],"metadata":{"id":"m7s1kCCFnwXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Before one hot encoding: {input_seq_int_enc[0]}, Shape: {len(input_seq_int_enc[0])}\")\n","input_seq_onehot = one_hot_encoding(input_seq_int_enc, char2int)\n","print(f\"After one hot encoding: {input_seq_onehot[0]}, Shape: {input_seq_onehot[0].shape}\")"],"metadata":{"id":"ul7FjWTOU0_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we're done with the pre-processing, let's convert types into `torch.Tensor`."],"metadata":{"id":"ow8Z5kmK4FdX"}},{"cell_type":"code","source":["input_seq_onehot = torch.from_numpy(input_seq_onehot)\n","target_seq_int_enc = torch.Tensor(target_seq_int_enc)"],"metadata":{"id":"Xdlr212xa3lT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model building\n","\n","We'll be defining the model using the `torch` library. We'll be using the basic `nn.rnn` to demonstrate a simple example of how RNNs can be used.\n","\n","To start building our own neural network, we can define a class that inherits PyTorch's base class (`nn.Module`) for all neural network modules.\n","\n","Our model class will:\n","  - have its layers initiated in the constructor.\n","  - particulary, use only 1 layer of RNN followed by a fully connected layer. The fully connected layer will convert the RNN output to our desired output shape.\n","  - contain a method to initialize a tensor of zeros for the hidden state, `init_hidden()`.\n","  - contain a forward pass method, `forward()`, which will be executed sequentially. We'll pass in the inputs and the zero-initialized hidden state first, then pass the intermediate result to the fully-connected layer.\n"],"metadata":{"id":"D9BERgTh4xN7"}},{"cell_type":"code","source":["class RNNModel(nn.Module):\n","  def __init__(self, input_dim, output_dim, hidden_dim, n_layers):\n","    super(RNNModel, self).__init__()\n","\n","    # Defining some parameters\n","    # ------------------------\n","    self.hidden_dim = hidden_dim\n","    self.n_layers = n_layers\n","\n","    # Layer definition\n","    # ----------------\n","    self.rnn = nn.RNN(input_dim, hidden_dim, n_layers, batch_first=True) # with 'batch_first=True', the input dim expected is (batch_size, seq_len, one_hot_encode_len)\n","    self.fc = nn.Linear(hidden_dim, output_dim)\n","  \n","  def init_hidden(self, batch_size):\n","    \"\"\"Generate a tensor with zeros with size of (n_layers, batch_size, hidden_dim).\n","    It will act as our initial hidden state.    \n","    \"\"\"\n","    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n","    return hidden\n","\n","  def forward(self, x):\n","    batch_size = x.size(0)\n","\n","    # initialize hidden state\n","    hidden = self.init_hidden(batch_size)\n","\n","    # Passing in the input and hidden state into the model\n","    out, hidden = self.rnn(x, hidden)\n","\n","    # Reshaping the output such that it can be fit into the  fully connected layer\n","    out = out.contiguous().view(-1, self.hidden_dim)\n","    out = self.fc(out)\n","    # contiguous() returns itself if the input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data\n","    # What is contiguous? https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n","\n","    return out, hidden"],"metadata":{"id":"ugX6F3Jb1i4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the loss function, we use `CrossEntropyLoss` as the final output is basically a number of possible characters, which make it a classification task. Also, we use the common Adam optimizer.\n","\n","Next, we'll instantate the model with the relevant parameters and define our hyper-parameters which are epochs and learning rate."],"metadata":{"id":"dZpCZjDcVrAU"}},{"cell_type":"code","source":["# Model instantiation\n","model_rnn = RNNModel(input_dim = len(char2int), output_dim=len(char2int), hidden_dim=12, n_layers=1)\n","\n","# Print model summary\n","print(model_rnn)"],"metadata":{"id":"px0rsrjZVo1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training the model"],"metadata":{"id":"Ycs0v29-XCBv"}},{"cell_type":"code","source":["n_epochs = 100\n","lr = 0.01\n","\n","# Initialize the optimizer and loss function\n","optimizer = optim.Adam(model_rnn.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(n_epochs):\n","  optimizer.zero_grad() # Clears existing gradients from previous epoch\n","  output, hidden = model_rnn(input_seq_onehot)\n","  loss = criterion(output, target_seq_int_enc.view(-1).long())\n","  loss.backward() # Calculate gradient and backprop\n","  optimizer.step() # Update the weights accordingly\n","\n","  if epoch %10 == 0:\n","    print(f\"Epoch: {epoch}/{n_epochs}.....\", end=\" \")\n","    print(f\"Loss: {loss.item():.4f}\")"],"metadata":{"id":"xvwbjZdRXA0Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Text\n","\n","Let's test out model now and see what kind of output will we get. A helper function to convert our model ouptut back to text will be helpful!"],"metadata":{"id":"UGUUZiQ6aa4G"}},{"cell_type":"code","source":["def predict(model, input_characters):\n","  \"\"\"Returns the predicted character as well as the hidden state.\"\"\"\n","\n","  # One hot encoding our input characters to fit in the model\n","  input_characters_int_enc = integer_encoding(input_characters, char2int)\n","  input_characters_onehot = one_hot_encoding(input_characters_int_enc, char2int)\n","  input_characters_onehot = torch.from_numpy(np.array(input_characters_onehot)) # Convert to torch tensor\n","\n","  out, hidden = model(input_characters_onehot)\n","\n","  # Pass into Softmax to output probability\n","  # Only take the last output, which does not include our input\n","  prob = F.softmax(out[-1], dim=0).data\n","  # Taking the class with the highest probability score from the output\n","  char_ind = torch.max(prob, dim=0)[1].item()\n","\n","  return int2char[char_ind], hidden"],"metadata":{"id":"w2sjZuy5Zjre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample(model, sentence_length, starting_chars):\n","  model.eval()\n","  # Put starting chars in a list\n","  chars = list(starting_chars)\n","  predict_size = sentence_length - len(chars)\n","  # Now pass in the previous characters and get a new one\n","  for _ in range(predict_size):\n","    input_characters = []\n","    input_characters.append(chars)\n","    char, h = predict(model,input_characters)\n","    chars.append(char)\n","  \n","  return ''.join(chars)"],"metadata":{"id":"uWzqM-OtdSZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before we test, let's remind ourselves on the sentences that the model was trained with and the length of each sentence."],"metadata":{"id":"FpGRqyTOocsf"}},{"cell_type":"code","source":["print(text)\n","print(len(text[0]))"],"metadata":{"id":"T28NbfJOoiCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample(model=model_rnn, sentence_length=15, starting_chars='hey')"],"metadata":{"id":"1bSI9NWthDce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhxF3A5B6zNJ"},"source":["\n","## Model Limitations"]},{"cell_type":"markdown","metadata":{"id":"eXemw0v46zNJ"},"source":["1. Overfitting\n","    - We only fed the model with a few sentences during training, therefore it essentially “memorized” the sequence of characters of these sentences and thus returned us the exact sentence that we trained it on. <br>\n","    <br>\n","2. Handling of unseen characters\n","    - The model is currently only able to process the characters that it has seen before in the training data set. <br>\n","    <br>\n","3. Representation of Textual Data\n","\n","    - In this implementation, we used one-hot encoding to represent our characters. While it may be fine for this task due to its simplicity, most of the time it should not be used as a solution in actual or more complex problems. This is because:\n","\n","        - It is computationally too expensive for large datasets. \n","        - There is no contextual/semantic information embedded in one-hot vectors.\n","\n","    - Instead, most modern NLP solutions rely on word embeddings (word2vec, GloVe) or more recently, unique contextual word representations in BERT, ELMo, and ULMFit. You can also use the pretrained NLP models which will ease the task instead of you having to train a new model on millions of existing words."]},{"cell_type":"markdown","metadata":{"id":"-Pd9fcRo6zNJ"},"source":["While the vanilla RNN is rarely used in solving NLP or sequential problems nowadays, having a good grasp of the basic concepts of RNNs will definitely aid in your understanding as you move towards the more popular GRUs and LSTMs."]},{"cell_type":"markdown","metadata":{"id":"dVYGFpxC6zNJ"},"source":["## Appendix"]},{"cell_type":"markdown","metadata":{"id":"1akOgrhK6zNK"},"source":["##### Revolutionized Google Search Engine"]},{"cell_type":"markdown","metadata":{"id":"haOQ8gR66zNK"},"source":["On 18-20 May 2021, Google hosted its annual Google I/O conference. They presented updates to existing technologies such as Google Maps or Google Photos together with some amazing technologies such as LaMDA, a skilled conversationalist AI that could revolutionize chatbot tech, or MUM. \n","\n","MUM is an improvement of Google’s search engine. Like other popular state-of-the-art language models such as GPT-3 or LaMDA, MUM is based on the transformer architecture. BERT (MUM’s predecessor) is similar in this regard, the main difference being that MUM is 1000x more powerful. \n","\n","<p style='text-align: right;'> --- by Alberto Romero, dated 29 May 2021 <a href=\"https://towardsdatascience.com/will-googles-mum-kill-seo-d283927f0fde\">source</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"sK4CoPWI6zNK"},"source":["## Reference\n","\n","1. [A Beginner’s Guide on Recurrent Neural Networks with PyTorch](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n","2. [Pytorch [Basics] — Intro to RNN](https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677)"]}]}