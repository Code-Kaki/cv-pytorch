{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ8_rATdS42e"
   },
   "source": [
    "<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n",
    "\n",
    "> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program and the accompanying materials are made available under the\n",
    "terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "License for the specific language governing permissions and limitations\n",
    "under the License. <br>\n",
    "<br>**SPDX-License-Identifier: Apache-2.0** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQU3N2O1SJ1e"
   },
   "source": [
    "# Convolutional Neural Network: Fruits Classification Using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p7vVokzU_iC"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this hands-on, we will be building a fruits classifier using a **CNN pretrained model**. We will only classify the images of the fruits into 3 classes namely **apple, grapes and lemon**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r4Zn0fxTOBp"
   },
   "source": [
    "## Objectives\n",
    "In this hands-on, we will :-\n",
    "\n",
    "1. Download the fruits image dataset from a provided link.\n",
    "2. Make the dataset iterable by using it as a DataLoader object.\n",
    "5. Instantiate a pretrained Model (VGG) class.\n",
    "6. Instantiate the Loss class.\n",
    "7. Instantiate the Optimizer class.\n",
    "8. Train the CNN Model.\n",
    "9. Visualize metrics of the CNN Model.\n",
    "10. Save and load the CNN Model.\n",
    "11. Classify a test image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l07AlkjpU0Bn"
   },
   "source": [
    "## Transfer Learning \n",
    "\n",
    "The key motivation of transfer learning is that most models which solve complex problems need a lot of data train on in order to perform well, especially for deep learning. However, getting large dataset for a specific domain is hard.\n",
    "\n",
    "**Transfer learning** enables us to reuse knowledge from previously learned tasks and apply them to the new related ones. Instead of training the a new neural network from scratch, we “transfer” the learned features and use the model trained on other similar problem as a starting point.\n",
    "\n",
    "A **pre-trained model** is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. The intuition behind is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. \n",
    "\n",
    "In practice, very few people train an CNN from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to use a pretrained CNN on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the CNN either as an initialization or a fixed feature extractor for the task of interest. Read more about transfer learning [here](https://cs231n.github.io/transfer-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvjBbn7yX_6j"
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPE0oqrdTJmr"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib import request\n",
    "import zipfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSOMk6MgX9f-"
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "  def update_to(self, b=1, bsize=1, tsize=None):\n",
    "    if tsize is not None: self.total = tsize\n",
    "    self.update(b * bsize - self.n)\n",
    "\n",
    "download_link = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip'\n",
    "DATASET_BASE_PATH = Path(\"../datasets/FruitsClassification\").resolve()\n",
    "\n",
    "if not DATASET_BASE_PATH.exists(): DATASET_BASE_PATH.mkdir()\n",
    "\n",
    "destination_file = Path.joinpath(DATASET_BASE_PATH, \"fruits_classification_zip.zip\")\n",
    "if not destination_file.exists():\n",
    "  with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=download_link.split('/')[-1]) as t:\n",
    "    request.urlretrieve(download_link, destination_file, reporthook=t.update_to)\n",
    "  zipr = zipfile.ZipFile(destination_file)\n",
    "  zipr.extractall(DATASET_BASE_PATH)\n",
    "  zipr.close()\n",
    "else:\n",
    "  print(f\"{destination_file} already exists, skipping download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ukjzdNrbxZt"
   },
   "source": [
    "Open the `fruit_classification_data` folder in your root directory and take a look at the folder structure and the images within them.\n",
    "\n",
    "In the first level, there are four subfolders namely:-\n",
    "- train      : Images used to train the model\n",
    "- test       : Images used to test the model\n",
    "- validation : Images used to validate the model\n",
    "- dirty_test : Images that represent real life input which may be more challenging for a classifier model.\n",
    "\n",
    "In the second level: there are three subfolders, one for each class of:-\n",
    "- apple\n",
    "- grapes\n",
    "- lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PF7y1hVZOJM"
   },
   "outputs": [],
   "source": [
    "data_dir = Path.joinpath(DATASET_BASE_PATH, \"fruits_image_classification\")\n",
    "train_dir = Path.joinpath(data_dir,\"train\")\n",
    "valid_dir = Path.joinpath(data_dir,\"validation\")\n",
    "test_dir = Path.joinpath(data_dir,\"test\")\n",
    "dirtytest_dir = Path.joinpath(data_dir,\"dirty_test\")\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCwKZ3zGoMWg"
   },
   "source": [
    "## Data Augmentation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cqCkF0ToRB_"
   },
   "source": [
    "The idea of data augmentation is to artificially increase the number of training images by applying random transformations to the images. For each epoch, a different random transformation is applied to each training image. \n",
    "\n",
    "Data augmentation is used to improve robustness of deep learning model. It helps reducing overfitting and improves generalization.\n",
    "\n",
    "Note that each pre-trained model will have different input requirements, but if we read through what Imagenet requires, we figure out that our images need to be 224x224 and normalized to a range shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDTfls4UvQXz"
   },
   "source": [
    "Quick References\n",
    "\n",
    "1. Why use those specific numbers in image normalization? [ref1](https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670/7) [ref2](https://pytorch.org/vision/stable/models.html)\n",
    "-https://arxiv.org/abs/1409.1556\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfIeiW0KoQuR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj1L1bVCuLKJ"
   },
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFgo9G8-dHqk"
   },
   "outputs": [],
   "source": [
    "image_transforms = {\n",
    "    # Augment data only for the training dataset\n",
    "    \"train\": transforms.Compose([\n",
    "                                 transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomRotation(degrees=(-15,15)),\n",
    "                                 transforms.ColorJitter(),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize( \n",
    "                                     mean=[0.485, 0.456, 0.406], # All pretrained models expect input image to be normalized with these setting\n",
    "                                     std=[0.229, 0.224, 0.225] # Refer to the Quick Reference section above for more details\n",
    "                                     )\n",
    "    ]),\n",
    "    \"valid\":  transforms.Compose([\n",
    "                                  transforms.Resize(256),\n",
    "                                  transforms.CenterCrop(224),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(\n",
    "                                     mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]\n",
    "                                     )\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNS4jhgExYGX"
   },
   "source": [
    "Since our dataset directory satisfies the convention of the directory tree stated in the official documentation of [torchvision.datasets.ImageFolder](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageFolder), we can care less when using this method to read our input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BPq8t1Ot-E0"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train\": datasets.ImageFolder(root = train_dir, transform=image_transforms[\"train\"]),\n",
    "    \"valid\": datasets.ImageFolder(root = valid_dir, transform=image_transforms[\"valid\"])\n",
    "}\n",
    "\n",
    "batch_size = 10\n",
    "data_loaders = {\n",
    "    \"train\": DataLoader(data[\"train\"], batch_size = batch_size, shuffle=True),\n",
    "    \"valid\": DataLoader(data[\"valid\"], batch_size = batch_size, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS5SE2J-2BYw"
   },
   "source": [
    "Let's view the images in the dataloader. You will notice that the transformations has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT5bMcqg0YJI"
   },
   "outputs": [],
   "source": [
    "# To check the iterative behavior of the DataLoader (Optional)\n",
    "# Iterate through the dataloader once\n",
    "features, labels = next(iter(data_loaders[\"train\"]))\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgyhq3S33YyR"
   },
   "source": [
    "The shape of the features show that there are 10 samples in each iteration of the `DataLoader` object, whereby each iteration holds an RGB image sized 224 x 224.\n",
    "\n",
    "And as expected there are 10 labels, one for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQSi36BU3YMP"
   },
   "outputs": [],
   "source": [
    "# Visualization of the samples in the first iteration of the DataLoader object\n",
    "grid = torchvision.utils.make_grid(features, nrow=10)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(np.transpose(grid,(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1yAdC4uta3z"
   },
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_CsQ7TRtdHU"
   },
   "source": [
    "PyTorch has a models subpackage that contains model architectures and pretrained models that have been trained on 1000 classes of images (millions of images) in Imagenet. See the list of models available in `torchvision.models` [here](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "For this example, we’ll be using the VGG-16. Assuming the given number of training samples (images) are not sufficient enough for building a classifier, in this case, VGG can be easily leveraged for feature extraction as it is trained on millions of images. Though it didn’t record the lowest error and has a higher inference time due to large number of parameters, it is quicker to train than other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GgjbNyOtjcl"
   },
   "source": [
    "Here are the general steps to use a pre-trained model:\n",
    "- Load in pre-trained weights / model trained on a large dataset.\n",
    "- Freeze all the weights in the front (convolutional) layers (adjusting layers to freeze based on the similarity of new task to the trained large dataset).\n",
    "- Replace the end of the network with a custom classifier (set the number of outputs to be the number of classes).\n",
    "- Train only on the unfreezed layers for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrBW2RVRty5m"
   },
   "source": [
    "### Instantiate VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mt_mYxMasBOp"
   },
   "outputs": [],
   "source": [
    "vgg16_model = models.vgg16(pretrained=True)\n",
    "vgg16_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQZr1BZxxzuZ"
   },
   "source": [
    "Notice that there are three major section in VGG16's architecture. \n",
    "- `features`\n",
    "- `avgpool`\n",
    "- `classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52sNZxYRuAdD"
   },
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for name, param in vgg16_model.named_parameters():\n",
    "  print(name, param.numel())\n",
    "  total_params += param.numel()\n",
    "print(\"------------------\")\n",
    "print(\"Total parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zwq-ZdapvNzz"
   },
   "source": [
    "This model with almost 140 million parameters had already been trained with 1.3 million images! This greatly shortens our time to develop a fruits classifier model. \n",
    "\n",
    "For us to be able to use the trained features as our fruits classifier we need to make several modifications since we are using a model that was developed for 1000 classes. We will freeze all the layers to retain their weights and biases but replace entirely the final layer in the `classifier` sequential layers - `classifier[6]` which is a fully-connected layer with 4096 input neurons and 1000 output neurons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGKaYbOEu6ye"
   },
   "outputs": [],
   "source": [
    "# Freeze model weights and biases\n",
    "for param in vgg16_model.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypYu3JjZys41"
   },
   "outputs": [],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Before modification\")\n",
    "print(\"--------------\")\n",
    "print(f\"{vgg16_model.classifier}\\n\")\n",
    "\n",
    "# Custom create a few additional layers to replace classifier[6]\n",
    "new_layers = nn.Sequential(\n",
    "    nn.Linear(4096, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "vgg16_model.classifier[6] = new_layers\n",
    "print(\"--------------\")\n",
    "print(\"After modification\")\n",
    "print(\"--------------\")\n",
    "print(f\"{vgg16_model.classifier}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWP69Wf_4meE"
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in vgg16_model.parameters())\n",
    "print(f\"total_param = {total_params}\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in vgg16_model.parameters() if p.requires_grad)\n",
    "print(f'total_trainable_params = {total_trainable_params}')\n",
    "print(f\"{round(total_trainable_params/total_params*100, 2)}% of the model's parameters require training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HF9r2mL_pU4"
   },
   "source": [
    "Now that we had added a new layer, let's train the trainable parameters using our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpIQuPl08IlI"
   },
   "outputs": [],
   "source": [
    "# Instantiate optimizer and loss function\n",
    "optimizer = torch.optim.SGD(\n",
    "    [\n",
    "     {\"params\": vgg16_model.features.parameters()},\n",
    "     {\"params\": vgg16_model.classifier.parameters(), \"lr\": 1e-3}\n",
    "    ], \n",
    "    lr=1e-2, \n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup Hyperparameters\n",
    "epochs = 60\n",
    "n_epochs_stop = 10\n",
    "loss_score = {\"train\": [], \"valid\": []}\n",
    "\n",
    "# Setup Tensorboard writer\n",
    "TENSORBOARD_LOGS_PATH = Path(\"./run_VGG16_Fruits\").resolve()\n",
    "writer = SummaryWriter(TENSORBOARD_LOGS_PATH)\n",
    "\n",
    "# Setup model saving path\n",
    "MODEL_SAVE_BASE = Path(\"../generated_models\").resolve()\n",
    "if not MODEL_SAVE_BASE.exists(): MODEL_SAVE_BASE.mkdir() # Create folder\n",
    "MODEL_SAVE_PATH = Path.joinpath(MODEL_SAVE_BASE, \"VGG16_TL_FruitClassifier.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsSFUt_7IBXh"
   },
   "source": [
    "Use GPU, if available, else just use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRZ4wGx-Eyay"
   },
   "outputs": [],
   "source": [
    "# Check availability of CUDA, cuDNN and check model\n",
    "# --STRONGLY RECOMMEND TO USE GPU, ELSE IT'LL BE AGES FOR YOUR TRAINING TO COMPLETE!--\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available and torch.backends.cudnn.enabled:\n",
    "  print(\"GPU is available, training will use GPU instead of CPU.\")\n",
    "  # Move model and variables to gpu\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  vgg16_model.to(device)\n",
    "  criterion.to(device)\n",
    "else:\n",
    "  print(\"GPU is unavailable, training remains in the default CPU setting.\")\n",
    "\n",
    "## (OPTIONAL): Verify the device the model parameters are at by uncommenting the following line\n",
    "# next(vgg16_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1f-3a4uHY6U"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, model_saving_path, writer, n_epochs_stop, epochs):\n",
    "  epochs_no_improve = 0\n",
    "  min_val_loss = np.Inf\n",
    "  early_stop = False\n",
    "  curr_epoch = 0\n",
    "\n",
    "  while curr_epoch < epochs and early_stop is False:\n",
    "  \n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "      running_loss = 0.0\n",
    "      running_size = 0.0\n",
    "      epoch_loss = 0.0\n",
    "      correct = 0\n",
    "\n",
    "      if phase == \"train\":\n",
    "        model.train()\n",
    "      else:\n",
    "        # Set layers to evaluation model before running inference\n",
    "        model.eval()\n",
    "      \n",
    "      for images, labels in loader[phase]:\n",
    "        # Move tensor to GPU if available\n",
    "        if device: # device=None if 'cpu'\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Predict and compute loss\n",
    "        with torch.set_grad_enabled(phase==\"train\"):\n",
    "          y_pred = model(images)\n",
    "          loss = criterion(y_pred, labels)\n",
    "\n",
    "          if phase == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # If data size is not divisible by batch size\n",
    "        # The last batch size will be the remainder, thus, we multiply the loss with the number of items in batch \n",
    "        running_loss += loss.item()*y_pred.size(0)\n",
    "        running_size += y_pred.size(0)\n",
    "\n",
    "        # The predictions is the index of the maximum values in the output of model\n",
    "        predictions = torch.max(y_pred, 1)[1]\n",
    "        correct += (predictions == labels).sum().item()\n",
    "      \n",
    "      # Calculate the epoch_loss (running_size is equal to the respective data size)\n",
    "      epoch_loss = running_loss / running_size\n",
    "      epoch_accuracy = correct / running_size\n",
    "      writer.add_scalars('Loss', {phase: epoch_loss}, curr_epoch)\n",
    "      writer.add_scalars('Accuracy', {phase: epoch_accuracy}, curr_epoch)\n",
    "\n",
    "      # Print score every 5 epochs\n",
    "      if (curr_epoch % 5 == 0):\n",
    "        if phase == 'train':\n",
    "            print(f'Epoch {curr_epoch}:')\n",
    "        print(f'  {phase.upper()} Loss: {epoch_loss}')\n",
    "        print(f'  {phase.upper()} Accuracy: {epoch_accuracy}')\n",
    "\n",
    "      loss_score[phase].append(epoch_loss)\n",
    "\n",
    "      # Early stopping\n",
    "      if phase == 'valid':\n",
    "        if epoch_loss < min_val_loss:\n",
    "          # Save the model before the epochs start to not improving\n",
    "          torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "          print(f\"Model saved at Epoch {curr_epoch} \\n\")\n",
    "          epochs_no_improve = 0\n",
    "          min_val_loss = epoch_loss\n",
    "\n",
    "        else:\n",
    "          # Add 1 to epochs_no_improve if valid epoch_loss is not lower then min_val_loss\n",
    "          epochs_no_improve += 1\n",
    "          print(f'\\tepochs_no_improve: {epochs_no_improve} at Epoch: {curr_epoch}')\n",
    "          \n",
    "          if epochs_no_improve == n_epochs_stop:\n",
    "            print(f'\\n Early stopping condition was met when there were no improvements in validation loss for {n_epochs_stop} epochs, continuously!')\n",
    "            early_stop = True\n",
    "      \n",
    "    curr_epoch += 1\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMFtH-o9NaUN"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training with early-stopping technique\n",
    "train(vgg16_model, data_loaders, optimizer, criterion, MODEL_SAVE_PATH, writer, n_epochs_stop, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF7-mRAY5Acs"
   },
   "source": [
    "The cell output already displays a lot of information in the form of text. You can also make use of `Tensorboard` to visualize the accuracies and losses per iteration.\n",
    "\n",
    "If you're using **Windows** and are not able to view your plots after running the following cell, there is a temporary workaround [(Reference)](https://github.com/tensorflow/tensorboard/issues/2481#issuecomment-516974546). Run the following commands in `CMD.exe` or `Powershell` and try running the cell again:-\n",
    ">`taskkill /im tensorboard.exe /f`</br>\n",
    ">`del /q %TMP%\\.tensorboard-info\\*` (CMD.exe only)</br>\n",
    ">`del $env:TMP\\.tensorboard-info\\*` (Powershell only)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYlfTeW_6SXZ"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={TENSORBOARD_LOGS_PATH.as_posix()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjb8V6whVFc-"
   },
   "source": [
    "### Model inferencing\n",
    "\n",
    "We know that our model does well on both the training and validation dataset, but the ultimate determinant of its performance is on how it scores against the hold-out set that it has not seen before.\n",
    "\n",
    "Before we start inferencing, let's assume we only have  the best model that we've saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aKA3394TOLe"
   },
   "outputs": [],
   "source": [
    "# Load the best model's state_dict by first instantiating the VGG16 model, excluding it's pretrained weights and biases.\n",
    "model_inference = models.vgg16()\n",
    "model_inference.classifier[6] = new_layers\n",
    "model_inference.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsFHUZb6WIV-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the test images from test_dir\n",
    "test_data = datasets.ImageFolder(test_dir, transform=image_transforms['valid'])\n",
    "\n",
    "batch_size = len(test_data)\n",
    "test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
    "\n",
    "# moving model_inference to GPU if available\n",
    "if device:\n",
    "  model_inference.to(device)\n",
    "\n",
    "for images, labels in test_loader:\n",
    "  correct = 0\n",
    "  if device:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "  y_pred = model_inference(images)\n",
    "  predictions = torch.max(y_pred, 1)[1]\n",
    "  print(predictions)\n",
    "  print(labels)\n",
    "  correct += (predictions == labels).sum().item()\n",
    "  accuracy = correct / len(test_data)\n",
    "  print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhEm96SNagad"
   },
   "source": [
    "Note that we have provided an extra test set named `dirty_test` containing images that are harder to classify. Let's test our model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYq__B3Iaqfi"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the test images from test_dir\n",
    "dirtytest_data = datasets.ImageFolder(dirtytest_dir, transform=image_transforms['valid'])\n",
    "\n",
    "batch_size = len(dirtytest_data)\n",
    "dirtytest_loader = DataLoader(dirtytest_data, batch_size, shuffle=False)\n",
    "\n",
    "try:\n",
    "  # moving model_inference to GPU if available\n",
    "  if device:\n",
    "    model_inference.to(device)\n",
    "\n",
    "  for images, labels in dirtytest_loader:\n",
    "    correct = 0\n",
    "    if device:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "    y_pred = model_inference(images)\n",
    "    predictions = torch.max(y_pred, 1)[1]\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    correct += (predictions == labels).sum().item()\n",
    "    accuracy = correct / len(dirtytest_data)\n",
    "    print(f\"Dirty Test Accuracy: {accuracy}\")\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "  # Use CPU to inference if GPU lack in memory\n",
    "  # moving model to CPU\n",
    "  device = torch.device(\"cpu\")\n",
    "  if device:\n",
    "    model_inference.to(device)\n",
    "    print(f\"Moved model to {device}\")\n",
    "\n",
    "  for images, labels in dirtytest_loader:\n",
    "    correct = 0\n",
    "    if device:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "    y_pred = model_inference(images)\n",
    "    predictions = torch.max(y_pred, 1)[1]\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    correct += (predictions == labels).sum().item()\n",
    "    accuracy = correct / len(dirtytest_data)\n",
    "    print(f\"Dirty Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO0tbLxd2sYy"
   },
   "source": [
    "Optionally, you may try to build your own CNN model and compare its performance with the VGG16 pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khGY9WDrmqMU"
   },
   "source": [
    "### Appendix <a id=appendix></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLvsJNa6mqMU"
   },
   "source": [
    "**PyTorch Hub Pre-trained Model**\n",
    "\n",
    "A recent announcement in the PyTorch world provides an additional route to get models: PyTorch Hub. This is supposed to become a central location for obtaining any published model in the future, whether it’s for operating on not only images, like in the torchvision model library, but all types like **images, text, audio, video, or any other type of data**. To obtain a model in this fashion, you use the `torch.hub module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uNT5_6fmqMU"
   },
   "outputs": [],
   "source": [
    "# resnet_model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-otntjv_mqMV"
   },
   "source": [
    "The first parameter points to a GitHub owner and repository (with an optional tag/\n",
    "branch identifier in the string as well); the second is the model requested (in this case,\n",
    "resnet50); and finally, the third indicates whether to download pretrained weights.\n",
    "You can also use `torch.hub.list('pytorch/vision')` to discover all the models\n",
    "inside that repository that are available to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AuzFTETmqMV"
   },
   "source": [
    "### Reference\n",
    "1. [Transfer Learning with Convolutional Neural Networks in PyTorch](https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce)\n",
    "2. [Transfer Learning - Machine Learning's Next Frontier](https://ruder.io/transfer-learning/)\n",
    "3. [Transfer Learning and Fine-tuning](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "4. [Deep Learning for Everyone: Master the Powerful Art of Transfer Learning using PyTorch](https://www.analyticsvidhya.com/blog/2019/10/how-to-master-transfer-learning-using-pytorch/?utm_source=blog&utm_medium=building-image-classification-models-cnn-pytorch)\n",
    "5. [A Poor Example of Transfer Learning: Applying VGG Pre-trained model with Keras](https://towardsdatascience.com/a-demonstration-of-transfer-learning-of-vgg-convolutional-neural-network-pre-trained-model-with-c9f5b8b1ab0a)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBQO+GRdtHf3a+mxGnRmFi",
   "collapsed_sections": [],
   "name": "02_CNN_FruitClassification.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
