{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ8_rATdS42e"
   },
   "source": [
    "<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n",
    "\n",
    "> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program and the accompanying materials are made available under the\n",
    "terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "License for the specific language governing permissions and limitations\n",
    "under the License. <br>\n",
    "<br>**SPDX-License-Identifier: Apache-2.0** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85zQOHOkUAtN"
   },
   "source": [
    "# Image Segmentation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object segmentation is the process of finding the boundaries of target objects in images. There are many applications for segmenting objects in an image. For example, by outlining anatomical objects in medical images, clinical experts can learn useful information about patient's conditions. The goal of automatic single-object segmentation is to predict a binary mask given in an image, whereby the object of interest will be in white and the background is black. \n",
    "\n",
    "<img src=\"https://rumc-gcorg-p-public.s3.amazonaws.com/b/265/bannerV3_V5OH10E.x15.jpeg\"></img>\n",
    "\n",
    "This hands-on will guide you through building a pipeline to automatically segment fetal head in ultrasound images, from scratch.\n",
    "\n",
    "_Authored by: [Scotrraaj Gopal](http://github.com/scotgopal)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this hands-on, we will :-\n",
    "\n",
    "1. Download the fetal head dataset.\n",
    "2. Create a custom Dataset object.\n",
    "3. Define the deep learning model.\n",
    "4. Define the loss function and optimizer.\n",
    "5. Train the model.\n",
    "6. Test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "download_links = [\"https://zenodo.org/record/1322001/files/training_set.zip?download=1\",\"https://zenodo.org/record/1322001/files/test_set.zip?download=1\"]\n",
    "destination_files = [\"training_set.zip\", \"test_set.zip\"]\n",
    "DATASET_BASE_PATH = Path(\"../datasets/FetalHeadDataset\").resolve()\n",
    "\n",
    "if not DATASET_BASE_PATH.exists(): DATASET_BASE_PATH.mkdir()\n",
    "\n",
    "for download_link, destination_file in zip(download_links, destination_files):\n",
    "    destination_file = Path.joinpath(DATASET_BASE_PATH, destination_file)\n",
    "    if not destination_file.exists():\n",
    "        with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=download_link.split('/')[-1]) as t:\n",
    "            request.urlretrieve(download_link, destination_file, reporthook=t.update_to)\n",
    "        zipr = zipfile.ZipFile(destination_file)\n",
    "        zipr.extractall(DATASET_BASE_PATH)\n",
    "        zipr.close()\n",
    "    else:\n",
    "        print(f\"{destination_file} already exists, skipping download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the source of the documentation [here](https://hc18.grand-challenge.org/), this dataset was posted as part of a challenge to enable automated fetal head segmentation. The description includes information that the training set contains 800x540 images; 999 for the training set and 335 for the test set. The training set includes an image of the **manual annotation** of the head circumference for each image. \n",
    "\n",
    "Let's verify it by first opening the downloaded dataset directory and viewing the data itself. If you had opened the downloaded dataset, you'd realize that there are many `.png` files in both `training_set` and `test_set` folders, but the targets' file names are prefixed with \"_Annotation\". Let's programmatically verify the counts of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dir = Path.joinpath(DATASET_BASE_PATH, \"training_set\")\n",
    "feature_image_list = []\n",
    "target_image_list = []\n",
    "for path in list(train_val_dir.glob(\"*\")):\n",
    "    if \".png\" in path.name:\n",
    "        if \"Annotation\" in path.name:\n",
    "            target_image_list.append(path)\n",
    "        else:\n",
    "            feature_image_list.append(path)\n",
    "\n",
    "print(\"Total training images:\", len(feature_image_list))\n",
    "print(\"Total target images:\", len(target_image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a small sample of these training images along with their annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.rcParams[\"figure.figsize\"] = [17,10]\n",
    "\n",
    "total_images_to_display = 3\n",
    "random_image_paths = np.random.choice(feature_image_list, total_images_to_display)\n",
    "corresponding_target_paths = [str(image_path).replace(\".png\", \"_Annotation.png\") for image_path in random_image_paths]\n",
    "\n",
    "def get_masked_image(feature_image, target_image):\n",
    "    \"\"\"Function to plot the boundaries of the binary mask onto the input image\"\"\"\n",
    "    # mark_boundaries returns a 3-channel image, ranging from 0 - 1\n",
    "    normalized_masked_image = mark_boundaries(np.array(feature_image), np.array(target_image), color=(0,1,0), mode=\"thick\")\n",
    "    masked_image = (normalized_masked_image*255).astype(np.uint8)\n",
    "    return masked_image\n",
    "\n",
    "for feature_image, target_image in zip(random_image_paths, corresponding_target_paths):\n",
    "    feature_image_pil = Image.open(feature_image)\n",
    "    target_image_pil = Image.open(target_image)\n",
    "    masked_image = get_masked_image(feature_image_pil, target_image_pil)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(feature_image_pil, cmap=\"gray\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(target_image_pil, cmap=\"gray\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many packages that we can choose from to perform transformation on images. There's the built-in `transforms` package in `torchvision`. There're also third-party packages such as [`Albumentations`](https://albumentations.ai/) and [`imgaug`](https://imgaug.readthedocs.io/en/latest/).\n",
    "\n",
    "In this hands-on, we will show you how to transform the dataset using `Albumentations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import Compose, Resize, HorizontalFlip, VerticalFlip\n",
    "\n",
    "height, width = 128, 192\n",
    "transform_train = Compose([Resize(height=height, width=width), HorizontalFlip(p=0.5), VerticalFlip(p=0.5)])\n",
    "transform_val = Compose([Resize(height=height, width=width)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate what each transformer does to an input feature image, annotation image and a masked image.\n",
    "\n",
    "_Note: Since there is an element of randomness (p) to each transformation, every run of the following cell will return different output._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_image, target_image in zip(random_image_paths, corresponding_target_paths):\n",
    "    feature_image_pil = Image.open(feature_image)\n",
    "    target_image_pil = Image.open(target_image)\n",
    "    masked_image = get_masked_image(feature_image_pil, target_image_pil)\n",
    "\n",
    "    transformer_output_dict = transform_train(image=np.array(feature_image_pil), mask=np.array(target_image_pil))\n",
    "    feature_image_transformed = transformer_output_dict['image']\n",
    "    target_image_transformed = transformer_output_dict['mask']\n",
    "    masked_image_transformed = get_masked_image(feature_image_transformed, target_image_transformed)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.imshow(feature_image_pil, cmap=\"gray\")\n",
    "    plt.title(\"feature image before transform\")\n",
    "\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.imshow(feature_image_transformed, cmap=\"gray\")\n",
    "    plt.title(\"feature image after transform\")\n",
    "\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.imshow(target_image_pil, cmap=\"gray\")\n",
    "    plt.title(\"target image before transform\")\n",
    "\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.imshow(target_image_transformed, cmap=\"gray\")\n",
    "    plt.title(\"target image after transform\")\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.imshow(masked_image, cmap=\"gray\")\n",
    "    plt.title(\"masked image before transform\")\n",
    "\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.imshow(masked_image_transformed, cmap=\"gray\")\n",
    "    plt.title(\"masked image after transform\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next, we are going to define a custom Dataset class to ease our process of extracting images in demand. Using `torch.utils.data.Dataset` as our base class, we will extend this class to create a custom child class, `FetalHead_Dataset`.\n",
    "\n",
    "Ideally, we will use this class to later create three objects; `train_ds`, `val_ds` and finally `test_ds`. We will split the images from the `training_set` folder into `train_ds` and `val_ds` to train our model. Then we will use `test_ds` to sample some images from `test_set` folder to see how our model performs.\n",
    "\n",
    "_Note: The images in the `test_set` folder were originally provided for participants of this challenge to have a standard way of testing their models and making a submission. We will just use the images from this folder to validate if the model is able to perform with unseen data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import ndimage\n",
    "\n",
    "class FetalHead_Dataset(Dataset):\n",
    "    def __init__(self, images_dir:Path, type, transform=None):\n",
    "        super().__init__()\n",
    "        _valid_types = [\"train\",\"val\",\"test\"]\n",
    "        if type not in _valid_types:\n",
    "            raise ValueError(f\"Invalid dataset type: '{type}'. Use one of these: {_valid_types}\")\n",
    "\n",
    "        self.images_dir = images_dir\n",
    "        self.type = type\n",
    "        self.transform = transform\n",
    "\n",
    "        self.feature_image_list = []\n",
    "        self.target_image_list = []        \n",
    "        for path in list(self.images_dir.glob(\"*\")):\n",
    "            if \".png\" in path.name:\n",
    "                if \"Annotation\" in path.name:\n",
    "                    self.target_image_list.append(path)\n",
    "                else:\n",
    "                    self.feature_image_list.append(path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature_image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature_image_path = self.feature_image_list[index]\n",
    "        feature_image_array = np.array(Image.open(feature_image_path))\n",
    "        feature_image_tensor = torch.tensor(feature_image_array, dtype=torch.uint8)\n",
    "\n",
    "        if self.type in [\"train\",\"val\"]:\n",
    "            target_image_path = self.target_image_list[index]\n",
    "            target_image_pil = Image.open(target_image_path)\n",
    "            target_image_filled = ndimage.binary_fill_holes(target_image_pil).astype(np.uint8)\n",
    "            target_image_tensor = torch.tensor(target_image_filled, dtype=torch.uint8)\n",
    "\n",
    "            if self.transform:\n",
    "                transformer_output_dict = self.transform(image=feature_image_array, mask=target_image_filled)\n",
    "                feature_image_transformed = transformer_output_dict[\"image\"]\n",
    "                target_image_transformed = transformer_output_dict[\"mask\"]\n",
    "                \n",
    "                feature_image_tensor = torch.tensor(feature_image_transformed, dtype=torch.uint8)\n",
    "                target_image_tensor = torch.tensor(target_image_transformed, dtype=torch.uint8)\n",
    "\n",
    "            return feature_image_tensor, target_image_tensor\n",
    "        else:\n",
    "            if self.transform:\n",
    "                transformer_output_dict = self.transform(image=feature_image_array)\n",
    "                feature_image_transformed = transformer_output_dict[\"image\"]\n",
    "\n",
    "                feature_image_tensor = torch.tensor(feature_image_transformed, dtype=torch.uint8)\n",
    "\n",
    "            return feature_image_tensor              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_val_dir)\n",
    "test_dir = Path.joinpath(DATASET_BASE_PATH, \"test_set\")\n",
    "print(test_dir)\n",
    "\n",
    "fetal_train_ds = FetalHead_Dataset(train_val_dir, \"train\", transform_train)\n",
    "fetal_val_ds = FetalHead_Dataset(train_val_dir, \"val\", transform_val)\n",
    "fetal_test_ds = FetalHead_Dataset(test_dir, \"test\", transform_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the properties of the three dataset objects created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_target = fetal_train_ds[0]\n",
    "train_masked = get_masked_image(train_feature, train_target)\n",
    "\n",
    "val_feature, val_target = fetal_val_ds[0]\n",
    "val_masked = get_masked_image(val_feature, val_target)\n",
    "\n",
    "test_feature = fetal_test_ds[0]\n",
    "\n",
    "print(\"Total training features: \" ,len(fetal_train_ds))\n",
    "print(\"-\"*5, \"Train Feature\", \"-\"*5)\n",
    "print(\"Shape:\", train_feature.shape, \"Type:\", train_feature.dtype, \"Max:\", train_feature.max())\n",
    "print(\"-\"*5, \"Train Target\", \"-\"*5)\n",
    "print(\"Shape:\", train_target.shape, \"Type:\", train_target.dtype, \"Max:\", train_target.max())\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Train feature\")\n",
    "plt.imshow(train_feature, cmap=\"gray\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Train target\")\n",
    "plt.imshow(train_target, cmap=\"gray\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Train masked\")\n",
    "plt.imshow(train_masked, cmap=\"gray\")\n",
    "\n",
    "print(\"\\nTotal validation features: \" ,len(fetal_val_ds))\n",
    "print(\"-\"*5, \"Val Feature\", \"-\"*5)\n",
    "print(\"Shape:\", val_feature.shape, \"Type:\", val_feature.dtype, \"Max:\", val_feature.max())\n",
    "print(\"-\"*5, \"Val Target\", \"-\"*5)\n",
    "print(\"Shape:\", val_target.shape, \"Type:\", val_target.dtype, \"Max:\", val_target.max())\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Val feature\")\n",
    "plt.imshow(val_feature, cmap=\"gray\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Val target\")\n",
    "plt.imshow(val_target, cmap=\"gray\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Val masked\")\n",
    "plt.imshow(val_masked, cmap=\"gray\")\n",
    "\n",
    "print(\"\\nTotal test features: \" ,len(fetal_test_ds))\n",
    "print(\"-\"*5, \"Test Feature\", \"-\"*5)\n",
    "print(\"Shape:\", test_feature.shape, \"Type:\", test_feature.dtype, \"Max:\", test_feature.max())\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Test feature\")\n",
    "plt.imshow(test_feature, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following:-\n",
    "\n",
    "1. All three datasets has the same `dtypes`.\n",
    "2. Features range from **0-255** while targets range from **0-1**.\n",
    "3. Training and validation dataset are almost identical including their total length and even the indices of the images (no shuffling done), but the images undergo different types of transformation.\n",
    "4. Test dataset undergoes the same type of transformation as the validation dataset.\n",
    "5. Test dataset has no targets.\n",
    "6. The target images have been alterred. The binary mask has now been filled using `skimage.segmentation.ndimage.binary_fill_holes` method. This is done because an object can be easily detected in an image if the object has sufficient contrast from it's background. [_Ref_](https://www.researchgate.net/publication/285371663_Image_Segmentation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will split the training dataset in such a way that `fetal_train_ds` has 8 parts and `fetal_val_ds` has 2 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "indice_range = range(len(fetal_train_ds))\n",
    "train_indices, val_indices = next(shuffle_split.split(indice_range))\n",
    "\n",
    "fetal_train_ds = Subset(fetal_train_ds, train_indices)\n",
    "fetal_val_ds = Subset(fetal_val_ds, val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same inspection as before, excluding the `fetal_test_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_target = fetal_train_ds[0]\n",
    "train_masked = get_masked_image(train_feature, train_target)\n",
    "\n",
    "val_feature, val_target = fetal_val_ds[0]\n",
    "val_masked = get_masked_image(val_feature, val_target)\n",
    "\n",
    "print(\"Total training features: \" ,len(fetal_train_ds))\n",
    "print(\"-\"*5, \"Train Feature\", \"-\"*5)\n",
    "print(\"Shape:\", train_feature.shape, \"Type:\", train_feature.dtype, \"Max:\", train_feature.max())\n",
    "print(\"-\"*5, \"Train Target\", \"-\"*5)\n",
    "print(\"Shape:\", train_target.shape, \"Type:\", train_target.dtype, \"Max:\", train_target.max())\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Train feature\")\n",
    "plt.imshow(train_feature, cmap=\"gray\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Train target\")\n",
    "plt.imshow(train_target, cmap=\"gray\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Train masked\")\n",
    "plt.imshow(train_masked, cmap=\"gray\")\n",
    "\n",
    "print(\"\\nTotal validation features: \" ,len(fetal_val_ds))\n",
    "print(\"-\"*5, \"Val Feature\", \"-\"*5)\n",
    "print(\"Shape:\", val_feature.shape, \"Type:\", val_feature.dtype, \"Max:\", val_feature.max())\n",
    "print(\"-\"*5, \"Val Target\", \"-\"*5)\n",
    "print(\"Shape:\", val_target.shape, \"Type:\", val_target.dtype, \"Max:\", val_target.max())\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Val feature\")\n",
    "plt.imshow(val_feature, cmap=\"gray\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Val target\")\n",
    "plt.imshow(val_target, cmap=\"gray\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Val masked\")\n",
    "plt.imshow(val_masked, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any overlap/ data leakage between `fetal_train_ds` and `fetal_val_ds`\n",
    "# True: no overlap, False: there is overlap\n",
    "set(fetal_train_ds.indices).intersection(set(fetal_val_ds.indices)) == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "fetal_train_dl = DataLoader(fetal_train_ds, batch_size=8, shuffle=True)\n",
    "fetal_val_dl = DataLoader(fetal_val_ds, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the properties of the sample batch provided in the first iteration of these dataloader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_image_batch, target_image_batch = next(iter(fetal_train_dl))\n",
    "print(\"-\"*5,\"fetal_train_dl\", \"-\"*5 )\n",
    "print(feature_image_batch.shape, feature_image_batch.dtype)\n",
    "print(target_image_batch.shape, target_image_batch.dtype)\n",
    "\n",
    "feature_image_batch, target_image_batch = next(iter(fetal_val_dl))\n",
    "print(\"-\"*5,\"fetal_val_dl\", \"-\"*5 )\n",
    "print(feature_image_batch.shape, feature_image_batch.dtype)\n",
    "print(target_image_batch.shape, target_image_batch.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the popular model architecture for segmentation tasks is the so-called **encoder-decoder** model.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png\" width=\"500\"></img></p>\n",
    "\n",
    "The first half of the model (encoding section) downsizes the feature map using a few layers of convolutional neural networks (CNN) and pooling layers. In the second half, the feature map is upsampled until it's the same size as the original image size to produce a binary mask. This model was later improved based on the concept of skip connections from `ResNet` to another popular architecture called `U-Net`. \n",
    "\n",
    "Let's proceed to define the model class and instantiating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self, params) -> None:\n",
    "        super().__init__()\n",
    "        C_in, H_in, W_in = params[\"input_shape\"]\n",
    "        init_f = params[\"initial_filters\"]\n",
    "        num_outputs = params[\"num_outputs\"]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2 * init_f, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(2 * init_f, 4 * init_f, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(4 * init_f, 8 * init_f, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(8 * init_f, 16 * init_f, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        self.conv_up1 = nn.Conv2d(16 * init_f, 8 * init_f, kernel_size=3, padding=1)\n",
    "        self.conv_up2 = nn.Conv2d(8 * init_f, 4 * init_f, kernel_size=3, padding=1)\n",
    "        self.conv_up3 = nn.Conv2d(4 * init_f, 2 * init_f, kernel_size=3, padding=1)\n",
    "        self.conv_up4 = nn.Conv2d(2 * init_f, init_f, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(init_f, num_outputs, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.conv_up1(x))\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.conv_up2(x))\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.conv_up3(x))\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.conv_up4(x))\n",
    "\n",
    "        x = self.conv_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model_params = {\n",
    "    \"input_shape\":(1,height,width), # the shape of the input data in a tuple-format: (channel, height, width)\n",
    "    \"initial_filters\": 16,          # the number of filters for the first CNN layer\n",
    "    \"num_outputs\": 1                # only 1 class for single object segmentation (binary)\n",
    "}\n",
    "model = SegNet(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# UNCOMMENT THE FOLLOWING LINE IF ENFORCE CPU ONLY\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the layers of the model using `print(model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain insights such as the output shape for each layer, the total number of trainable and non-trainable parameters including the estimated size of the model (`Params size`) using a 3rd party package, `torchsummary.summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1,height, width), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we already have a dataset loaded in data loader objects and we have already instantiated a model object and transferred the model to our GPU, if it was available. Now, to train this model, we need a loss function and an optimizer to update the model parameters based on the gradients of the loss.\n",
    "\n",
    "The classical loss function for single-object segmentation is the binary cross-entropy (BCE) loss function. The BCE loss function compares each pixel of the prediction with that of the ground truth; however, we can combine multiple criteria to improve the overall performance of the segmentation tasks. A popular technique is to **combine the dice metric with BCE loss**. The dice metrics is commonly used to test the performance of segmentation algorithms by calculating the amount of overlap between the ground truth and the prediction.\n",
    "\n",
    "Let's begin by first defining a function to calculate dice metric for a batch of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_batch(predicted_batch, target_batch, smooth=1e-5):\n",
    "    \"\"\"Function to calculate the dice loss per data batch\"\"\"\n",
    "    predicted_batch = torch.sigmoid(predicted_batch) # make all logits positive with range 0-1\n",
    "    intersection = (predicted_batch * target_batch).sum(dim=(2,3))\n",
    "    union = predicted_batch.sum(dim=(2,3)) + target_batch.sum(dim=(2,3))\n",
    "    dice_coeff = 2.0 * (intersection + smooth)/ (union + smooth)\n",
    "    dice_loss = 1.0 - dice_coeff\n",
    "    return dice_loss.sum(), dice_coeff.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `dice_coeff` as a validation metric for our model. But our loss is not yet complete until we combine it with BCE loss. Next, let's define another function to calculate the combined loss (dice + BCE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_batch(predicted_batch, target_batch):\n",
    "    \"\"\"Function to calculate the combined loss (dice + BCE) and per data batch\"\"\"\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(predicted_batch, target_batch, reduction=\"sum\")\n",
    "    dice_loss, _ = dice_loss_batch(predicted_batch, target_batch)\n",
    "    combined_loss = bce_loss + dice_loss\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could have returned `dice_coeff` as part of the output of the `loss_function_batch`, we are making a design choice by choosing not to. It is more maintainable to have one function to return only loss and another function to return the validation metric, as it may get messy if we were to make changes to our loss function and validation metric in the future.\n",
    "\n",
    "Let's create a function to return the `dice_coeff` as a validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metric_batch(predicted_batch, target_batch):\n",
    "    \"\"\"Function to return the validation metric\"\"\"\n",
    "    _, dice_coeff = dice_loss_batch(predicted_batch, target_batch)\n",
    "    return dice_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. So now we have two amazingly helpful functions. \n",
    "1. A function to calculate loss for one batch of data\n",
    "2. A function to calculate the validation metric for one batch of data\n",
    "\n",
    "Let's incorporate these functions into this `loss_epoch` function to iterate completely through a dataset and calculate the average values for the losses and validation metrics. This function should be designed such a way that it is convenient for us to swap datasets (`DataLoader` objects) and other training parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def loss_epoch(\n",
    "    model: nn.Module,\n",
    "    loss_function_batch,\n",
    "    validation_metric_batch,\n",
    "    dataset_dl: DataLoader,\n",
    "    optimizer: optim.Optimizer=None,\n",
    "    device = torch.device(\"cpu\"),\n",
    "    sanity_check: bool = False, # A flag used to test the training pipeline, trains the model only for a single batch (or single iteration)\n",
    "):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    total_dataset_size = len(dataset_dl.dataset)\n",
    "\n",
    "    for feature_image_batch, target_image_batch in dataset_dl:\n",
    "        feature_image_batch = feature_image_batch.type(torch.float32).unsqueeze(1).to(device) # Makes the 3D shape input [8,128,192] to 4D shape [8,1,128,192]\n",
    "        target_image_batch = target_image_batch.type(torch.float32).unsqueeze(1).to(device)\n",
    "        predicted_image_batch = model(feature_image_batch)\n",
    "\n",
    "        batch_loss = loss_function_batch(predicted_image_batch, target_image_batch)\n",
    "        batch_metric = validation_metric_batch(predicted_image_batch, target_image_batch)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad(set_to_none=True) # sets the .grad attribute of each parameter (tensor) to None\n",
    "            batch_loss.backward() # computes the gradient for every parameter that has `requires_grad=True`\n",
    "            optimizer.step() # updates the parameters based on their `.grad` attribute\n",
    "\n",
    "        running_loss += batch_loss.detach()\n",
    "        running_metric += batch_metric.detach()\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    epoch_loss = running_loss / float(total_dataset_size) # calculate average loss over all batches\n",
    "    epoch_metric = running_metric / float(total_dataset_size) # calculate average validation metric over all batches\n",
    "\n",
    "    # using `.item()` because we will just be collecting the numerical values, not other attributes like `.grad_fn` etc.\n",
    "    # crucial step to avoid Out Of Memory issues. STORE ONLY WHAT YOU NEED! \n",
    "    return epoch_loss.item(), epoch_metric.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model is an iterative process. In every epoch, we would:-\n",
    "1. Extract a mini batch from the training dataset and feed into model.\n",
    "2. Calculate training loss and metric and update the model parameters accordingly.\n",
    "3. Extract a mini batch from the validation dataset and feed into model.\n",
    "4. Calculate validation loss and metric and [create a general checkpoint](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) if there is an improvement to the validation loss compared to the last best.\n",
    "5. Print all the necessary details derived from the epoch run.\n",
    "6. Repeat Step 1-5 for the next epoch.\n",
    "\n",
    "Now that we understand the flow, let's implement this flow in a function, `train_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_model(model, params):\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    loss_func = params[\"loss_func\"]\n",
    "    val_func = params[\"val_func\"]\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    train_dl = params[\"train_dl\"]\n",
    "    val_dl = params[\"val_dl\"]\n",
    "    sanity_check = params[\"sanity_check\"]\n",
    "    checkpoint_path = params[\"checkpoint_path\"]\n",
    "    device = params[\"device\"]\n",
    "\n",
    "    loss_history = {\"train\": [], \"val\": []}\n",
    "    val_metric_history = {\"train\": [], \"val\": []}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        epoch_train_loss, epoch_train_metric = loss_epoch(model, loss_func, val_func, train_dl, optimizer, device, sanity_check)\n",
    "        loss_history[\"train\"].append(epoch_train_loss)\n",
    "        val_metric_history[\"train\"].append(epoch_train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_val_loss, epoch_val_metric = loss_epoch(model, loss_func, val_func, val_dl, device=device, sanity_check=sanity_check)\n",
    "\n",
    "        loss_history[\"val\"].append(epoch_val_loss)\n",
    "        val_metric_history[\"val\"].append(epoch_val_metric)\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_val_loss\n",
    "                }, \n",
    "                checkpoint_path)\n",
    "            print(\"Saved a new checkpoint for the best model so far!\")\n",
    "\n",
    "        print(f\"epoch_train_loss: {epoch_train_loss:.6f} dice: {100*epoch_train_metric:.2f}\")\n",
    "        print(f\"epoch_val_loss: {epoch_val_loss:.6f} dice: {100*epoch_val_metric:.2f}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "    print(\"Loading best model weights!\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, val_metric_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we had done all the preparation necessary to train our model. Let's not wait any longer and begin training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_BASE = Path(\"../generated_models\").resolve()\n",
    "if not MODEL_SAVE_BASE.exists(): MODEL_SAVE_BASE.mkdir()\n",
    "\n",
    "# define parameters for train_model function\n",
    "num_epochs = 30\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "sanity_check = False\n",
    "checkpoint_path = Path.joinpath(MODEL_SAVE_BASE, f\"ENC_DEC_Segmentation_{num_epochs}.pt\")\n",
    "\n",
    "params_train={\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"loss_func\": loss_function_batch,\n",
    "    \"val_func\": validation_metric_batch,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"train_dl\": fetal_train_dl,\n",
    "    \"val_dl\": fetal_val_dl,\n",
    "    \"sanity_check\": sanity_check,\n",
    "    \"checkpoint_path\": checkpoint_path,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "model, loss_history, val_metric_history = train_model(model, params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the historical loss and metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot historical values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train-Val Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), loss_history[\"train\"], label=\"train\")\n",
    "plt.plot(range(1, num_epochs + 1), loss_history[\"val\"], label=\"val\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train-Val Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), val_metric_history[\"train\"], label=\"train\")\n",
    "plt.plot(range(1, num_epochs + 1), val_metric_history[\"val\"], label=\"val\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the saved checkpoint to get more insights about the previous run. These information maybe useful to us for future experiments/ runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(\"Best Epoch:\", checkpoint[\"epoch\"])\n",
    "print(\"Best Loss:\", checkpoint[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we don't have a `model` variable. We'll instantiate a new model to test it's segmenting abilities with the data from the `test_set` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = SegNet(model_params)\n",
    "model_new.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model_new.eval()\n",
    "print(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images_to_test = 4\n",
    "random_indexes = np.random.choice(range(len(fetal_test_ds)),total_images_to_test)\n",
    "\n",
    "for id in random_indexes:\n",
    "    feature_image_tensor = fetal_test_ds[id]\n",
    "    feature_image = feature_image_tensor.numpy()\n",
    "\n",
    "    # preprocess tensor input to dtype:torch.float32 and shape:from 2D to 4D\n",
    "    feature_image_tensor = feature_image_tensor.type(torch.float32).view(1,1,128,192)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_image_tensor = model_new(feature_image_tensor)\n",
    "\n",
    "    # post-process tensor output; shape: from 4D to 2d, make all values positive using sigmoid\n",
    "    # use thresholding to get a binary mask\n",
    "    predicted_image = torch.sigmoid(predicted_image_tensor.squeeze()) >= 0.5\n",
    "    \n",
    "    masked_image = get_masked_image(feature_image, predicted_image)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(feature_image, cmap=\"gray\")\n",
    "    plt.title(\"Feature image\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(predicted_image, cmap=\"gray\")\n",
    "    plt.title(\"Predicted image\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(masked_image, cmap=\"gray\")\n",
    "    plt.title(\"Masked image\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4489ab8aa3dee1d47b1206c53587757c6e1af4a6bbea54dda3d3136caec07042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('conda-pytorch3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
