{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ8_rATdS42e"
   },
   "source": [
    "<img src=\"https://futurejobs.my/wp-content/uploads/2021/05/d-min-1024x297.png\" width=\"300\"> </img>\n",
    "\n",
    "> **Copyright &copy; 2021 Skymind Education Group Sdn. Bhd.**<br>\n",
    "> <br>\n",
    "> This program and the accompanying materials are made available under the\n",
    "> terms of the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \\\n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "> WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "> License for the specific language governing permissions and limitations\n",
    "> under the License. <br>\n",
    "> <br>**SPDX-License-Identifier: Apache-2.0**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85zQOHOkUAtN"
   },
   "source": [
    "# Object Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Object Detection intro here]\n",
    "\n",
    "<img src=\"\"></img>\n",
    "\n",
    "This hands-on will guide you through building a pipeline to automatically detect objects from the Pascal VOC Dataset, using a pretrained Single Shot Detector (SSD) model.\n",
    "\n",
    "_Authored by: [Scotrraaj Gopal](http://github.com/scotgopal)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import Compose, Resize, BboxParams\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "def process_and_transform(image, target, new_image_size=(400,400)):\n",
    "    def get_bbox_and_label_list(target_list_of_dicts):\n",
    "        bbox_list_of_dicts = [det_obj['bndbox'] for det_obj in target_list_of_dicts]\n",
    "        label_list = [det_obj['name'] for det_obj in target_list_of_dicts]\n",
    "        bbox_list_of_lists = []\n",
    "        for bbox_dict in bbox_list_of_dicts:\n",
    "            xmin = int(bbox_dict['xmin'])\n",
    "            ymin = int(bbox_dict['ymin'])\n",
    "            xmax = int(bbox_dict['xmax'])\n",
    "            ymax = int(bbox_dict['ymax'])\n",
    "            bbox_list_of_lists.append([xmin, ymin, xmax, ymax])\n",
    "        return bbox_list_of_lists, label_list\n",
    "\n",
    "    target = target['annotation']['object']\n",
    "    bbox_list_of_lists, label_list = get_bbox_and_label_list(target)\n",
    "    albu_transformer = Compose([Resize(*new_image_size)], bbox_params=BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "    transformed_dict = albu_transformer(image=np.array(image), bboxes=bbox_list_of_lists, class_labels=label_list)\n",
    "    transformed_img = to_tensor(transformed_dict['image'])\n",
    "    transformed_target = [{'name':name, 'bbox':bbox_list} for name,bbox_list in zip(transformed_dict['class_labels'],transformed_dict['bboxes'])]\n",
    "    return transformed_img, transformed_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE_PATH = Path(\"../datasets\").resolve()\n",
    "VOC_DATASET_DIR = Path.joinpath(DATASET_BASE_PATH, \"VOCdevkit\")\n",
    "if VOC_DATASET_DIR.exists():\n",
    "    trainval_ds = datasets.VOCDetection(root=\"../datasets\", image_set=\"trainval\", transforms=process_and_transform)\n",
    "    \n",
    "else:\n",
    "    trainval_ds = datasets.VOCDetection(root=\"../datasets\", image_set=\"trainval\", transforms=process_and_transform, download=True)\n",
    "\n",
    "trainval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = trainval_ds[4]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "feature_tensor, target_annotation = trainval_ds[200]\n",
    "feature_image = to_pil_image(feature_tensor)\n",
    "for annotated_object in target_annotation:\n",
    "    name = annotated_object['name']\n",
    "    xmin, ymin, xmax, ymax = annotated_object['bbox']\n",
    "\n",
    "    upper_left_point = (int(xmin), int(ymin))\n",
    "    lower_right_point = (int(xmax), int(ymax))\n",
    "    print(upper_left_point, lower_right_point)\n",
    "    colour = (255,0,0)\n",
    "    line_thickness = 2\n",
    "\n",
    "    org = (int(xmin), int(ymin)-5)\n",
    "    font = cv.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_colour = (255,0,0)\n",
    "    text_line_type = cv.LINE_AA\n",
    "    \n",
    "\n",
    "    feature_image = cv.rectangle(np.array(feature_image), upper_left_point, lower_right_point, colour, line_thickness)\n",
    "    feature_image = cv.putText(img=feature_image, text=name, org=org, fontFace=font, fontScale=font_scale,color=font_colour, lineType=text_line_type)\n",
    "\n",
    "plt.figure(figsize=(19,7))\n",
    "plt.imshow(feature_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "model.eval() # Put to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "indice_range = range(len(trainval_ds))\n",
    "train_indices, val_indices = next(shuffle_split.split(indice_range))\n",
    "\n",
    "voc_train_ds = Subset(trainval_ds, train_indices)\n",
    "voc_val_ds = Subset(trainval_ds, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset(feature_tensor, target_annotation):\n",
    "    feature_image = to_pil_image(feature_tensor)\n",
    "    for annotated_object in target_annotation:\n",
    "        name = annotated_object['name']\n",
    "        xmin, ymin, xmax, ymax = annotated_object['bbox']\n",
    "\n",
    "        upper_left_point = (int(xmin), int(ymin))\n",
    "        lower_right_point = (int(xmax), int(ymax))\n",
    "        print(f\"\\n{name}, {upper_left_point=}, {lower_right_point=}\")\n",
    "        colour = (255,0,0)\n",
    "        line_thickness = 2\n",
    "\n",
    "        org = (int(xmin), int(ymin)+10)\n",
    "        font = cv.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_colour = (255,0,0)\n",
    "\n",
    "        feature_image = cv.rectangle(np.array(feature_image), upper_left_point, lower_right_point, colour, line_thickness)\n",
    "        feature_image = cv.putText(img=feature_image, text=name, org=org, fontFace=font, fontScale=font_scale,color=font_colour, lineType=cv.LINE_AA, bottomLeftOrigin=False)\n",
    "\n",
    "    plt.imshow(feature_image)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "def inference(model, ds:Dataset, ds_index: int, det_threshold: float=0.45):\n",
    "    image_tensor_3d, annotation = ds[ds_index]\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    display_dataset(image_tensor_3d, annotation)\n",
    "    plt.title(\"My image/annotation\")\n",
    "    \n",
    "\n",
    "    # make inference on image\n",
    "    with torch.no_grad():\n",
    "        pred = model([image_tensor_3d])\n",
    "    \n",
    "    print(f\"\\n{len(pred)=}\")\n",
    "    for id, detections_dict in enumerate(pred):\n",
    "        print(\"\\nImage:\", id)\n",
    "        print(\"Total keys:\", len(detections_dict))\n",
    "        print(\"Keys:\", detections_dict.keys())\n",
    "        print(\"Total Scores:\", len(detections_dict['scores']))\n",
    "        print(\"Boxes\", len(detections_dict['boxes']))\n",
    "        print(\"First 5 Scores:\", detections_dict['scores'][:5])\n",
    "        print(\"First 5 Labels:\", detections_dict['labels'][:5])\n",
    "        detections_that_matter = (detections_dict['scores'][:20]>=det_threshold).nonzero(as_tuple=True)[0]\n",
    "        labels_that_matter = [detections_dict['labels'][index].item() for index in detections_that_matter.numpy()]\n",
    "        boxes_that_matter = [detections_dict['boxes'][index].tolist() for index in detections_that_matter.numpy()]\n",
    "        target_annotation = list()\n",
    "        for coco_index,bbox in zip(labels_that_matter, boxes_that_matter):\n",
    "            object = dict()\n",
    "            object['name'] = COCO_INSTANCE_CATEGORY_NAMES[coco_index]\n",
    "            object['bbox'] = bbox\n",
    "            target_annotation.append(object)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        display_dataset(image_tensor_3d, target_annotation)\n",
    "        plt.title(\"SSD output image/annotation\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "inference(model, voc_val_ds, 404, det_threshold=0.2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "voc_val_dl = DataLoader(voc_val_ds, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(voc_val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import requests\n",
    "\n",
    "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar\"\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"VOCdevkit_18-May-2011.tar\", \"wb\").write(res.content)\n",
    "\n",
    "my_tarfile = tarfile.TarFile(\"VOCdevkit_18-May-2011.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tarfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CATEGORY_NAMES = ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Freeze params in the pretrained model by changing every parameters' _.require_grad()_ property equals to **False**\n",
    "2. Design another output layer and replace the old output layer with the new one.\n",
    "3. Prep training pipeline\n",
    "    a. loss pipeline for batch\n",
    "    b. evaluation metric + loss function for batch\n",
    "    c. optimizer, optimizer.step\n",
    "    d. loss pipeline for epoch\n",
    "    e. save the model weights, epoch number, metrics, optimizer's state_dict (weights?) for everytime the model improves\n",
    "    f. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4489ab8aa3dee1d47b1206c53587757c6e1af4a6bbea54dda3d3136caec07042"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('conda-pytorch3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
